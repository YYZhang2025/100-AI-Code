{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b79be8",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Attention Implementation: Forward and Backward](#toc1_)    \n",
    "- 2. [FlashAttention PyTorch Implementation](#toc2_)    \n",
    "- 3. [FlashAttention Triton Implementation](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4884bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b987ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "B: int = 4\n",
    "H: int = 8\n",
    "S: int = 64\n",
    "D: int = 512\n",
    "\n",
    "assert D % H == 0, \"D must be divisible by H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fa709dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(\n",
    "    batch_size: int = B, n_heads: int = H, seq_len: int = S, d_model: int = D, require_grad: bool = True\n",
    "):\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "    head_dim = d_model // n_heads\n",
    "\n",
    "    q = torch.randn(batch_size, n_heads, seq_len, head_dim, requires_grad=require_grad)\n",
    "    k = torch.randn(batch_size, n_heads, seq_len, head_dim, requires_grad=require_grad)\n",
    "    v = torch.randn(batch_size, n_heads, seq_len, head_dim, requires_grad=require_grad)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "\n",
    "def reset_gradients(q, k, v):\n",
    "    q.grad = k.grad = v.grad = None  # reset gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97ba2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference implementation of scaled dot-product attention\n",
    "def scaled_dot_product_attention(q, k, v, is_causal=True):\n",
    "    d_k = q.size(-1)\n",
    "    scale = 1.0 / (d_k**0.5)\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # [B, H, T, T]\n",
    "\n",
    "    if is_causal:\n",
    "        T = q.size(-2)\n",
    "        causal_mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=q.device), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    return torch.matmul(attn_probs, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e226c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Union\n",
    "\n",
    "attention_impl_dict: dict[str, Union[Type[torch.autograd.Function], None]] = {\n",
    "    \"vanilla\": None,\n",
    "    \"pytorch\": None,\n",
    "    \"triton\": None,\n",
    "}\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attention_impl=\"vanilla\", is_causal=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert attention_impl in [\"vanilla\", \"pytorch\", \"triton\"]\n",
    "        assert attention_impl_dict[attention_impl] is not None, f\"{attention_impl} is not implemented\"\n",
    "\n",
    "        self.attention_impl = attention_impl_dict[attention_impl]\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        if self.attention_impl is None:\n",
    "            raise ValueError(\n",
    "                \"Attention implementation not set. Please initialize with a valid implementation.\"\n",
    "            )\n",
    "        return self.attention_impl.apply(q, k, v, self.is_causal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b1a287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multihead_attention_match(attention_impl=\"vanilla\"):\n",
    "    torch.manual_seed(42)\n",
    "    q, k, v = get_qkv()\n",
    "\n",
    "    attn = MultiHeadAttention(attention_impl=attention_impl)\n",
    "    out_attn = attn(q, k, v)\n",
    "\n",
    "    # Reference output\n",
    "    out_ref = scaled_dot_product_attention(q, k, v)\n",
    "    assert torch.allclose(out_attn, out_ref, atol=1e-5, rtol=1e-4), \"Output mismatch\"\n",
    "\n",
    "    # Gradient check\n",
    "    grad_output = torch.randn_like(out_attn)\n",
    "\n",
    "    reset_gradients(q, k, v)  # reset gradients before backward pass\n",
    "    out_attn.backward(grad_output, retain_graph=True)\n",
    "    dq_attn, dk_attn, dv_attn = q.grad.clone(), k.grad.clone(), v.grad.clone()\n",
    "\n",
    "    reset_gradients(q, k, v)  # reset gradients before backward pass\n",
    "    out_ref.backward(grad_output)\n",
    "    dq_ref, dk_ref, dv_ref = q.grad.clone(), k.grad.clone(), v.grad.clone()\n",
    "\n",
    "    assert torch.allclose(dq_attn, dq_ref, atol=1e-5, rtol=1e-4), \"dq mismatch\"\n",
    "    assert torch.allclose(dk_attn, dk_ref, atol=1e-5, rtol=1e-4), \"dk mismatch\"\n",
    "    assert torch.allclose(dv_attn, dv_ref, atol=1e-5, rtol=1e-4), \"dv mismatch\"\n",
    "\n",
    "    print(f\"✅ {attention_impl} matches reference scaled dot-product attention (output and gradients)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e11f5d",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>Attention Implementation: Forward and Backward [&#9757;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaAttention(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, causal=False) -> torch.Tensor:\n",
    "        d_k = q.size(-1)\n",
    "        s = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5\n",
    "\n",
    "        if causal:\n",
    "            mask = torch.tril(torch.ones(s.size(-2), s.size(-1), device=s.device))\n",
    "            s = s.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        p = F.softmax(s, dim=-1)\n",
    "        output = torch.matmul(p, v)\n",
    "\n",
    "        # Save the activation for the backward pass\n",
    "        ctx.save_for_backward(q, k, v, p)\n",
    "        ctx.causal = causal\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        q, k, v, p = ctx.saved_tensors\n",
    "        d_k = q.size(-1)\n",
    "        scale = 1.0 / (d_k**0.5)\n",
    "\n",
    "        s = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "        # Causal mask (optional)\n",
    "        if getattr(ctx, \"causal\", False):\n",
    "            T = s.size(-1)\n",
    "            causal_mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=s.device), diagonal=1)\n",
    "            s = s.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "\n",
    "        # Gradient wrt attention scores\n",
    "        grad_attn = torch.matmul(grad_output, v.transpose(-2, -1))\n",
    "\n",
    "        # Derivative of softmax\n",
    "        grad_scores = p * (grad_attn - (p * grad_attn).sum(dim=-1, keepdim=True))\n",
    "\n",
    "        grad_q = torch.matmul(grad_scores, k) * scale\n",
    "        grad_k = torch.matmul(grad_scores.transpose(-2, -1), q) * scale\n",
    "        grad_v = torch.matmul(p.transpose(-2, -1), grad_output)\n",
    "\n",
    "        return grad_q, grad_k, grad_v, None\n",
    "\n",
    "\n",
    "attention_impl_dict[\"vanilla\"] = VanillaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f04323b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ vanilla matches reference scaled dot-product attention (output and gradients)\n"
     ]
    }
   ],
   "source": [
    "test_multihead_attention_match(attention_impl=\"vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "545f5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def benchmark_attention(attention_impl, warmup=2, time_rounds=10):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    q = torch.randn(B, H, S, D, requires_grad=True)\n",
    "    k = torch.randn(B, H, S, D, requires_grad=True)\n",
    "    v = torch.randn(B, H, S, D, requires_grad=True)\n",
    "\n",
    "    attn = MultiHeadAttention(attention_impl=attention_impl)\n",
    "    grad_output = torch.randn_like(attn(q, k, v))\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(warmup):\n",
    "        out = attn(q, k, v)\n",
    "        out.backward(grad_output)\n",
    "        q.grad = k.grad = v.grad = None\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(time_rounds):\n",
    "        out = attn(q, k, v)\n",
    "        out.backward(grad_output)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        q.grad = k.grad = v.grad = None\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    avg_time_ms = (end - start) / time_rounds * 1000\n",
    "    print(f\"[{attention_impl}] Avg forward+backward time: {avg_time_ms:.2f} ms over {time_rounds} rounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8576eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ vanilla matches reference scaled dot-product attention (output and gradients)\n"
     ]
    }
   ],
   "source": [
    "test_multihead_attention_match(attention_impl=\"vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "384cf3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vanilla] Avg forward+backward time: 4.40 ms over 10 rounds\n"
     ]
    }
   ],
   "source": [
    "benchmark_attention(\"vanilla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5e90e",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>FlashAttention PyTorch Implementation [&#9757;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64810831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def _pad_to_multiple(x: torch.Tensor, size: int, dim: int, value: float = 0.0) -> Tuple[torch.Tensor, int]:\n",
    "    n = x.size(dim)\n",
    "    pad = (-n) % size\n",
    "    if pad == 0:\n",
    "        return x, n\n",
    "    pad_shape = list(x.shape)\n",
    "    pad_shape[dim] = pad\n",
    "    pad_tensor = x.new_full(pad_shape, value)\n",
    "    return torch.cat([x, pad_tensor], dim=dim), n\n",
    "\n",
    "\n",
    "class FlashAttentionPyTorchImpl(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, is_causal: bool = True, tile_size: int = 64):\n",
    "        \"\"\"\n",
    "        q, k, v: [B, H, N, D]  ->  O: [B, H, N, D]\n",
    "        Parallel across (B*H) and all query tiles; sequential over key tiles.\n",
    "        \"\"\"\n",
    "        assert q.dim() == k.dim() == v.dim() == 4\n",
    "        B, H, N_q, D = q.shape\n",
    "        _, _, N_k, Dk = k.shape\n",
    "        assert D == Dk and v.shape[-1] == D\n",
    "\n",
    "        device = q.device\n",
    "        out_dtype = q.dtype\n",
    "        cdtype = torch.float32\n",
    "        scale = D**-0.5\n",
    "\n",
    "        # Merge batch and heads\n",
    "        BH = B * H\n",
    "        q_bh = q.reshape(BH, N_q, D).to(cdtype)\n",
    "        k_bh = k.reshape(BH, N_k, D).to(cdtype)\n",
    "        v_bh = v.reshape(BH, N_k, D).to(cdtype)\n",
    "\n",
    "        B_q = B_k = tile_size\n",
    "\n",
    "        # Pad to multiples of tile sizes\n",
    "        q_pad, N_q_orig = _pad_to_multiple(q_bh, B_q, dim=1, value=0.0)  # [BH, T_q*B_q, D]\n",
    "        k_pad, N_k_orig = _pad_to_multiple(k_bh, B_k, dim=1, value=0.0)  # [BH, T_k*B_k, D]\n",
    "        v_pad, _ = _pad_to_multiple(v_bh, B_k, dim=1, value=0.0)  # [BH, T_k*B_k, D]\n",
    "\n",
    "        T_q = q_pad.size(1) // B_q\n",
    "        T_k = k_pad.size(1) // B_k\n",
    "\n",
    "        # Tile views\n",
    "        Q = q_pad.view(BH, T_q, B_q, D)  # [BH, T_q, B_q, D]\n",
    "        K = k_pad.view(BH, T_k, B_k, D)  # [BH, T_k, B_k, D]\n",
    "        V = v_pad.view(BH, T_k, B_k, D)  # [BH, T_k, B_k, D]\n",
    "\n",
    "        # Accumulators (per BH, per query row)\n",
    "        O = Q.new_zeros(BH, T_q, B_q, D)  # output (unnormalized)\n",
    "        l = Q.new_zeros(BH, T_q, B_q)  # running normalizer\n",
    "        m = Q.new_full((BH, T_q, B_q), -float(\"inf\"))  # running max\n",
    "\n",
    "        # Precompute causal indices\n",
    "        if is_causal:\n",
    "            q_base = torch.arange(T_q, device=device).view(1, T_q, 1, 1) * B_q\n",
    "            q_rows = q_base + torch.arange(B_q, device=device).view(1, 1, B_q, 1)  # [1,T_q,B_q,1]\n",
    "            k_base = torch.arange(B_k, device=device).view(1, 1, 1, B_k)  # [1,1,1,B_k]\n",
    "\n",
    "        # Scan over key tiles (vectorized over BH and T_q)\n",
    "        for j in range(T_k):\n",
    "            K_j = K[:, j]  # [BH, B_k, D]\n",
    "            V_j = V[:, j]  # [BH, B_k, D]\n",
    "\n",
    "            # Scores: [BH, T_q, B_q, B_k]\n",
    "            S = torch.einsum(\"btqd,bkd->btqk\", Q, K_j) * scale\n",
    "            if is_causal:\n",
    "                k_rows = (j * B_k) + k_base\n",
    "                causal = q_rows >= k_rows  # [1,T_q,B_q,B_k] -> broadcast to [BH,...]\n",
    "                S = torch.where(causal, S, torch.tensor(-float(\"inf\"), device=device))\n",
    "\n",
    "            # Online softmax update\n",
    "            m_new = torch.maximum(m, S.amax(dim=-1))  # [BH,T_q,B_q]\n",
    "            P = torch.exp(S - m_new[..., None])  # [BH,T_q,B_q,B_k]\n",
    "            exp_m = torch.exp(m - m_new)  # [BH,T_q,B_q]\n",
    "\n",
    "            l = exp_m * l + P.sum(dim=-1)  # [BH,T_q,B_q]\n",
    "            O = (exp_m[..., None] * O) + torch.einsum(\"btqk,bkd->btqd\", P, V_j)\n",
    "            m = m_new\n",
    "\n",
    "        # Normalize and drop padding\n",
    "        O = O / l[..., None]  # [BH,T_q,B_q,D]\n",
    "        O = O.view(BH, T_q * B_q, D)[:, :N_q_orig, :]\n",
    "        O_saved = O.contiguous()  # fp32 for backward\n",
    "        O_out = O_saved.view(B, H, N_q_orig, D).to(out_dtype)\n",
    "\n",
    "        # Log-normalizer L = logsumexp per row, needed in backward\n",
    "        L = (m + torch.log(l)).view(BH, T_q * B_q)[:, :N_q_orig].contiguous()  # [BH, N_q_orig]\n",
    "\n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(q_bh, k_bh, v_bh, O_saved, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        ctx.tile_size = tile_size\n",
    "        ctx.sizes = (B, H, N_q_orig, N_k_orig, D)\n",
    "        return O_out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        q_bh, k_bh, v_bh, O_saved, L = ctx.saved_tensors\n",
    "        is_causal = ctx.is_causal\n",
    "        tile_size = ctx.tile_size\n",
    "        B, H, N_q_orig, N_k_orig, D = ctx.sizes\n",
    "        device = q_bh.device\n",
    "        cdtype = q_bh.dtype  # fp32\n",
    "        out_dtype = grad_out.dtype\n",
    "\n",
    "        # Merge BH and cast to compute dtype\n",
    "        BH = B * H\n",
    "        dO = grad_out.reshape(B, H, N_q_orig, D).reshape(BH, N_q_orig, D).to(cdtype)\n",
    "\n",
    "        # Row-wise dot(dO, O) -> D_row: [BH, N_q_orig]\n",
    "        D_row = (dO * O_saved).sum(dim=-1)  # [BH, N_q_orig]\n",
    "\n",
    "        # Pad q/k/v/dO/L/D_row to tile sizes and view into tiles\n",
    "        B_q = B_k = tile_size\n",
    "        q_pad, _ = _pad_to_multiple(q_bh, B_q, dim=1, value=0.0)  # [BH, T_q*B_q, D]\n",
    "        k_pad, _ = _pad_to_multiple(k_bh, B_k, dim=1, value=0.0)  # [BH, T_k*B_k, D]\n",
    "        v_pad, _ = _pad_to_multiple(v_bh, B_k, dim=1, value=0.0)  # [BH, T_k*B_k, D]\n",
    "        dO_pad, _ = _pad_to_multiple(dO, B_q, dim=1, value=0.0)  # [BH, T_q*B_q, D]\n",
    "        L_pad, _ = _pad_to_multiple(L, B_q, dim=1, value=-float(\"inf\"))  # [BH, T_q*B_q]\n",
    "        D_pad, _ = _pad_to_multiple(D_row, B_q, dim=1, value=0.0)  # [BH, T_q*B_q]\n",
    "\n",
    "        T_q = q_pad.size(1) // B_q\n",
    "        T_k = k_pad.size(1) // B_k\n",
    "\n",
    "        Q = q_pad.view(BH, T_q, B_q, D)  # [BH,T_q,B_q,D]\n",
    "        K = k_pad.view(BH, T_k, B_k, D)  # [BH,T_k,B_k,D]\n",
    "        V = v_pad.view(BH, T_k, B_k, D)  # [BH,T_k,B_k,D]\n",
    "        dO = dO_pad.view(BH, T_q, B_q, D)  # [BH,T_q,B_q,D]\n",
    "        Lr = L_pad.view(BH, T_q, B_q)  # [BH,T_q,B_q]\n",
    "        Dr = D_pad.view(BH, T_q, B_q)  # [BH,T_q,B_q]\n",
    "\n",
    "        # Grad accumulators in tile shapes\n",
    "        dQ = torch.zeros_like(Q)\n",
    "        dK = torch.zeros_like(K)\n",
    "        dV = torch.zeros_like(V)\n",
    "\n",
    "        scale = D**-0.5\n",
    "\n",
    "        # Precompute causal indices\n",
    "        if is_causal:\n",
    "            q_base = torch.arange(T_q, device=device).view(1, T_q, 1, 1) * B_q\n",
    "            q_rows = q_base + torch.arange(B_q, device=device).view(1, 1, B_q, 1)\n",
    "            k_base = torch.arange(B_k, device=device).view(1, 1, 1, B_k)\n",
    "\n",
    "        # Scan over key tiles; vectorized over BH and T_q\n",
    "        for j in range(T_k):\n",
    "            K_j = K[:, j]  # [BH,B_k,D]\n",
    "            V_j = V[:, j]  # [BH,B_k,D]\n",
    "\n",
    "            # Recompute scores\n",
    "            S = torch.einsum(\"btqd,bkd->btqk\", Q, K_j) * scale  # [BH,T_q,B_q,B_k]\n",
    "            if is_causal:\n",
    "                k_rows = (j * B_k) + k_base\n",
    "                causal = q_rows >= k_rows\n",
    "                S = torch.where(causal, S, torch.tensor(-float(\"inf\"), device=device))\n",
    "\n",
    "            # Probabilities from saved row log-normalizers\n",
    "            P = torch.exp(S - Lr[..., None])  # [BH,T_q,B_q,B_k]\n",
    "\n",
    "            # dV_j\n",
    "            dV[:, j] += torch.einsum(\"btqk,btqd->bkd\", P, dO)  # sum over T_q,B_q\n",
    "\n",
    "            # dP and dS\n",
    "            dP = torch.einsum(\"btqd,bkd->btqk\", dO, V_j)  # [BH,T_q,B_q,B_k]\n",
    "            dS = P * (dP - Dr[..., None])  # [BH,T_q,B_q,B_k]\n",
    "\n",
    "            # dQ and dK_j\n",
    "            dQ += torch.einsum(\"btqk,bkd->btqd\", dS * scale, K_j)  # [BH,T_q,B_q,D]\n",
    "            dK[:, j] += torch.einsum(\"btqk,btqd->bkd\", dS * scale, Q)  # [BH,B_k,D]\n",
    "\n",
    "        # Reshape back, drop padding, cast to input dtype\n",
    "        dQ = dQ.view(BH, T_q * B_q, D)[:, :N_q_orig, :].contiguous().view(B, H, N_q_orig, D).to(out_dtype)\n",
    "        dK = dK.view(BH, T_k * B_k, D)[:, :N_k_orig, :].contiguous().view(B, H, N_k_orig, D).to(out_dtype)\n",
    "        dV = dV.view(BH, T_k * B_k, D)[:, :N_k_orig, :].contiguous().view(B, H, N_k_orig, D).to(out_dtype)\n",
    "\n",
    "        # Non-tensor args\n",
    "        return dQ, dK, dV, None, None\n",
    "\n",
    "\n",
    "attention_impl_dict[\"pytorch\"] = FlashAttentionPyTorchImpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cf873a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pytorch matches reference scaled dot-product attention (output and gradients)\n"
     ]
    }
   ],
   "source": [
    "test_multihead_attention_match(attention_impl=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "429499c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pytorch] Avg forward+backward time: 6.24 ms over 10 rounds\n"
     ]
    }
   ],
   "source": [
    "benchmark_attention(\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e20e99",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>FlashAttention Triton Implementation [&#9757;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install triton>=3.0.0\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "def _pad_to_multiple(x: torch.Tensor, size: int, dim: int, value: float = 0.0) -> Tuple[torch.Tensor, int]:\n",
    "    n = x.size(dim)\n",
    "    pad = (-n) % size\n",
    "    if pad == 0:\n",
    "        return x, n\n",
    "    pad_shape = list(x.shape)\n",
    "    pad_shape[dim] = pad\n",
    "    pad_tensor = x.new_full(pad_shape, value)\n",
    "    return torch.cat([x, pad_tensor], dim=dim), n\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _flash_fwd_kernel(\n",
    "    Q_ptr,\n",
    "    K_ptr,\n",
    "    V_ptr,\n",
    "    O_ptr,\n",
    "    L_ptr,\n",
    "    BH: tl.constexpr,\n",
    "    N_Q: tl.constexpr,\n",
    "    N_K: tl.constexpr,\n",
    "    D: tl.constexpr,\n",
    "    stride_q_b,\n",
    "    stride_q_n,\n",
    "    stride_q_d,\n",
    "    stride_k_b,\n",
    "    stride_k_n,\n",
    "    stride_k_d,\n",
    "    stride_v_b,\n",
    "    stride_v_n,\n",
    "    stride_v_d,\n",
    "    stride_o_b,\n",
    "    stride_o_n,\n",
    "    stride_o_d,\n",
    "    stride_l_b,\n",
    "    stride_l_n,\n",
    "    scale,\n",
    "    is_causal: tl.constexpr,\n",
    "    BLOCK_M: tl.constexpr,  # query rows per program\n",
    "    BLOCK_N: tl.constexpr,  # key rows loaded per iteration\n",
    "):\n",
    "    pid_bh = tl.program_id(0)  # which (B*H)\n",
    "    pid_tq = tl.program_id(1)  # which query-tile for that (BH)\n",
    "\n",
    "    # Offsets\n",
    "    offs_m = pid_tq * BLOCK_M + tl.arange(0, BLOCK_M)  # query row indices\n",
    "    offs_n = tl.arange(0, BLOCK_N)  # key row indices (tile-local)\n",
    "    offs_d = tl.arange(0, D)  # head dim\n",
    "\n",
    "    # Base pointers for this BH\n",
    "    Q_b = Q_ptr + pid_bh * stride_q_b\n",
    "    K_b = K_ptr + pid_bh * stride_k_b\n",
    "    V_b = V_ptr + pid_bh * stride_v_b\n",
    "    O_b = O_ptr + pid_bh * stride_o_b\n",
    "    L_b = L_ptr + pid_bh * stride_l_b\n",
    "\n",
    "    # Bounds masks\n",
    "    qmask = offs_m < N_Q\n",
    "\n",
    "    # Load Q tile: [BLOCK_M, D]\n",
    "    # Guard loads for out-of-bounds rows; zeros for padded rows\n",
    "    Q_tile = tl.where(\n",
    "        qmask[:, None] & (offs_d[None, :] < D),\n",
    "        tl.load(\n",
    "            Q_b + offs_m[:, None] * stride_q_n + offs_d[None, :] * stride_q_d,\n",
    "            mask=(qmask[:, None] & (offs_d[None, :] < D)),\n",
    "            other=0.0,\n",
    "        ),\n",
    "        0.0,\n",
    "    )\n",
    "    Q_tile = Q_tile.to(tl.float32)\n",
    "\n",
    "    # Online softmax accumulators\n",
    "    m_i = tl.full((BLOCK_M,), -float(\"inf\"), tl.float32)\n",
    "    l_i = tl.zeros((BLOCK_M,), tl.float32)\n",
    "    acc = tl.zeros((BLOCK_M, D), tl.float32)\n",
    "\n",
    "    # Precompute per-tile query absolute indices for causal mask\n",
    "    # q_abs: [BLOCK_M, 1]\n",
    "    if is_causal:\n",
    "        q_abs = offs_m[:, None]  # broadcast against k_abs later\n",
    "\n",
    "    # Loop over key tiles\n",
    "    for start_n in range(0, N_K, BLOCK_N):\n",
    "        k_idx = start_n + offs_n\n",
    "        kmask = k_idx < N_K\n",
    "\n",
    "        # Load K_j: [BLOCK_N, D], V_j: [BLOCK_N, D]\n",
    "        K_j = tl.where(\n",
    "            kmask[:, None] & (offs_d[None, :] < D),\n",
    "            tl.load(\n",
    "                K_b + k_idx[:, None] * stride_k_n + offs_d[None, :] * stride_k_d,\n",
    "                mask=(kmask[:, None] & (offs_d[None, :] < D)),\n",
    "                other=0.0,\n",
    "            ),\n",
    "            0.0,\n",
    "        ).to(tl.float32)\n",
    "        V_j = tl.where(\n",
    "            kmask[:, None] & (offs_d[None, :] < D),\n",
    "            tl.load(\n",
    "                V_b + k_idx[:, None] * stride_v_n + offs_d[None, :] * stride_v_d,\n",
    "                mask=(kmask[:, None] & (offs_d[None, :] < D)),\n",
    "                other=0.0,\n",
    "            ),\n",
    "            0.0,\n",
    "        ).to(tl.float32)\n",
    "\n",
    "        # S = Q @ K^T : [BLOCK_M, BLOCK_N]\n",
    "        S = tl.dot(Q_tile, tl.trans(K_j)) * scale\n",
    "\n",
    "        # Apply causal mask if requested: set invalid to -inf\n",
    "        if is_causal:\n",
    "            k_abs = k_idx[None, :]  # [1, BLOCK_N]\n",
    "            causal = q_abs >= k_abs  # [BLOCK_M, BLOCK_N]\n",
    "            S = tl.where(causal & kmask[None, :] & qmask[:, None], S, -float(\"inf\"))\n",
    "        else:\n",
    "            S = tl.where(kmask[None, :] & qmask[:, None], S, -float(\"inf\"))\n",
    "\n",
    "        # Online softmax update\n",
    "        m_new = tl.maximum(m_i, tl.max(S, axis=1))\n",
    "        P = tl.exp(S - m_new[:, None])\n",
    "        exp_m = tl.exp(m_i - m_new)\n",
    "\n",
    "        l_i = exp_m * l_i + tl.sum(P, axis=1)\n",
    "        acc = exp_m[:, None] * acc + tl.dot(P, V_j)\n",
    "        m_i = m_new\n",
    "\n",
    "    # Normalize\n",
    "    O_tile = acc / l_i[:, None]\n",
    "\n",
    "    # Write O and L (guarded by qmask)\n",
    "    tl.store(\n",
    "        O_b + offs_m[:, None] * stride_o_n + offs_d[None, :] * stride_o_d,\n",
    "        O_tile,\n",
    "        mask=qmask[:, None] & (offs_d[None, :] < D),\n",
    "    )\n",
    "    L_row = m_i + tl.log(l_i)\n",
    "    tl.store(L_b + offs_m * stride_l_n, L_row, mask=qmask)\n",
    "\n",
    "\n",
    "class FlashAttentionTritonImpl(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, is_causal: bool = True, block_m: int = 128, block_n: int = 128):\n",
    "        \"\"\"\n",
    "        q, k, v: [B, H, N, D] (fp16/bf16/fp32)\n",
    "        Returns O: [B, H, N, D] in the same dtype as q.\n",
    "        Triton forward (online softmax). Backward is vectorized PyTorch.\n",
    "        \"\"\"\n",
    "        assert q.dim() == 4 and k.dim() == 4 and v.dim() == 4\n",
    "        B, H, N_q, D = q.shape\n",
    "        _, _, N_k, Dk = k.shape\n",
    "        assert D == Dk and v.shape[-1] == D\n",
    "\n",
    "        device = q.device\n",
    "        out_dtype = q.dtype\n",
    "        # Flatten BH\n",
    "        BH = B * H\n",
    "\n",
    "        # Make contiguous [BH, N, D]\n",
    "        q_bh = q.contiguous().view(BH, N_q, D)\n",
    "        k_bh = k.contiguous().view(BH, N_k, D)\n",
    "        v_bh = v.contiguous().view(BH, N_k, D)\n",
    "\n",
    "        # We allow fp16/bf16 inputs but do math in fp32 inside kernel\n",
    "        # (Triton kernel casts to fp32).\n",
    "        O = torch.empty_like(q_bh, dtype=torch.float32, device=device)\n",
    "        L = torch.empty((BH, N_q), dtype=torch.float32, device=device)\n",
    "\n",
    "        # Strides (row-major: [BH, N, D])\n",
    "        stride_q_b, stride_q_n, stride_q_d = q_bh.stride()\n",
    "        stride_k_b, stride_k_n, stride_k_d = k_bh.stride()\n",
    "        stride_v_b, stride_v_n, stride_v_d = v_bh.stride()\n",
    "        stride_o_b, stride_o_n, stride_o_d = O.stride()\n",
    "        stride_l_b, stride_l_n = L.stride()\n",
    "\n",
    "        # Launch grid\n",
    "        grid = (BH, triton.cdiv(N_q, block_m))\n",
    "        scale = D**-0.5\n",
    "\n",
    "        _flash_fwd_kernel[grid](\n",
    "            q_bh,\n",
    "            k_bh,\n",
    "            v_bh,\n",
    "            O,\n",
    "            L,\n",
    "            BH,\n",
    "            N_q,\n",
    "            N_k,\n",
    "            D,\n",
    "            stride_q_b,\n",
    "            stride_q_n,\n",
    "            stride_q_d,\n",
    "            stride_k_b,\n",
    "            stride_k_n,\n",
    "            stride_k_d,\n",
    "            stride_v_b,\n",
    "            stride_v_n,\n",
    "            stride_v_d,\n",
    "            stride_o_b,\n",
    "            stride_o_n,\n",
    "            stride_o_d,\n",
    "            stride_l_b,\n",
    "            stride_l_n,\n",
    "            scale,\n",
    "            is_causal,\n",
    "            BLOCK_M=block_m,\n",
    "            BLOCK_N=block_n,\n",
    "            num_warps=4,  # tune\n",
    "            num_stages=2,  # tune\n",
    "        )\n",
    "\n",
    "        O_saved = O.contiguous()  # fp32 saved for backward\n",
    "        O_out = O_saved.view(B, H, N_q, D).to(out_dtype)\n",
    "\n",
    "        # Save for backward\n",
    "        ctx.save_for_backward(\n",
    "            q_bh.to(torch.float32), k_bh.to(torch.float32), v_bh.to(torch.float32), O_saved, L\n",
    "        )\n",
    "        ctx.is_causal = is_causal\n",
    "        ctx.sizes = (B, H, N_q, N_k, D)\n",
    "        ctx.blocks = (block_m, block_n)\n",
    "        return O_out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        # Vectorized backward (same as your PyTorch version, BH & T_q parallel).\n",
    "        q_bh, k_bh, v_bh, O_saved, L = ctx.saved_tensors\n",
    "        is_causal = ctx.is_causal\n",
    "        B, H, N_q, N_k, D = ctx.sizes\n",
    "        block_m, block_n = ctx.blocks\n",
    "        device = q_bh.device\n",
    "        cdtype = q_bh.dtype\n",
    "        out_dtype = grad_out.dtype\n",
    "        BH = B * H\n",
    "\n",
    "        dO = grad_out.reshape(B, H, N_q, D).reshape(BH, N_q, D).to(cdtype)\n",
    "        D_row = (dO * O_saved).sum(dim=-1)  # [BH, N_q]\n",
    "\n",
    "        # Tile views via padding\n",
    "        def pad(x, size, dim, value):\n",
    "            n = x.size(dim)\n",
    "            pad = (-n) % size\n",
    "            if pad == 0:\n",
    "                return x, n\n",
    "            pad_shape = list(x.shape)\n",
    "            pad_shape[dim] = pad\n",
    "            return torch.cat([x, x.new_full(pad_shape, value)], dim=dim), n\n",
    "\n",
    "        Qp, _ = pad(q_bh, block_m, 1, 0.0)\n",
    "        Kp, _ = pad(k_bh, block_n, 1, 0.0)\n",
    "        Vp, _ = pad(v_bh, block_n, 1, 0.0)\n",
    "        dOp, _ = pad(dO, block_m, 1, 0.0)\n",
    "        Lp, _ = pad(L, block_m, 1, -float(\"inf\"))\n",
    "        Dp, _ = pad(D_row, block_m, 1, 0.0)\n",
    "\n",
    "        T_q = Qp.size(1) // block_m\n",
    "        T_k = Kp.size(1) // block_n\n",
    "\n",
    "        Q = Qp.view(BH, T_q, block_m, D)\n",
    "        K = Kp.view(BH, T_k, block_n, D)\n",
    "        V = Vp.view(BH, T_k, block_n, D)\n",
    "        dO = dOp.view(BH, T_q, block_m, D)\n",
    "        Lr = Lp.view(BH, T_q, block_m)\n",
    "        Dr = Dp.view(BH, T_q, block_m)\n",
    "\n",
    "        dQ = torch.zeros_like(Q)\n",
    "        dK = torch.zeros_like(K)\n",
    "        dV = torch.zeros_like(V)\n",
    "\n",
    "        scale = D**-0.5\n",
    "\n",
    "        # Precompute causal indices\n",
    "        if is_causal:\n",
    "            q_base = torch.arange(T_q, device=device).view(1, T_q, 1, 1) * block_m\n",
    "            q_rows = q_base + torch.arange(block_m, device=device).view(1, 1, block_m, 1)\n",
    "            k_base = torch.arange(block_n, device=device).view(1, 1, 1, block_n)\n",
    "\n",
    "        for j in range(T_k):\n",
    "            K_j = K[:, j]  # [BH, Bk, D]\n",
    "            V_j = V[:, j]  # [BH, Bk, D]\n",
    "            # S: [BH, Tq, Bm, Bk]\n",
    "            S = torch.matmul(Q, K_j.transpose(-2, -1)) * scale\n",
    "            if is_causal:\n",
    "                k_rows = (j * block_n) + k_base\n",
    "                causal = q_rows >= k_rows\n",
    "                S = torch.where(causal, S, torch.tensor(-float(\"inf\"), device=device))\n",
    "\n",
    "            P = torch.exp(S - Lr[..., None])\n",
    "\n",
    "            # dV\n",
    "            dV[:, j] += torch.matmul(P.transpose(-2, -1), dO)  # [BH,Bk,D]\n",
    "\n",
    "            # dS\n",
    "            dP = torch.matmul(dO, V_j.transpose(-2, -1))  # [BH,Tq,Bm,Bk]\n",
    "            dS = P * (dP - Dr[..., None])  # [BH,Tq,Bm,Bk]\n",
    "\n",
    "            # dQ, dK\n",
    "            dQ += torch.matmul(dS * scale, K_j)  # [BH,Tq,Bm,D]\n",
    "            dK[:, j] += torch.matmul(dS.transpose(-2, -1) * scale, Q)  # [BH,Bk,D]\n",
    "\n",
    "        # Reshape & cast\n",
    "        dQ = dQ.view(BH, T_q * block_m, D)[:, :N_q, :].contiguous().view(B, H, N_q, D).to(out_dtype)\n",
    "        dK = dK.view(BH, T_k * block_n, D)[:, :N_k, :].contiguous().view(B, H, N_k, D).to(out_dtype)\n",
    "        dV = dV.view(BH, T_k * block_n, D)[:, :N_k, :].contiguous().view(B, H, N_k, D).to(out_dtype)\n",
    "        return dQ, dK, dV, None, None\n",
    "\n",
    "\n",
    "# ---- Registry wiring\n",
    "attention_impl_dict[\"triton\"] = FlashAttentionTritonImpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adc9b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, time\n",
    "\n",
    "\n",
    "# def bench(fn, reps=50, warmup=10):\n",
    "#     for _ in range(warmup):\n",
    "#         fn()\n",
    "#     torch.cuda.synchronize()\n",
    "#     t0 = time.time()\n",
    "#     for _ in range(reps):\n",
    "#         fn()\n",
    "#     torch.cuda.synchronize()\n",
    "#     return (time.time() - t0) / reps\n",
    "\n",
    "\n",
    "# B, H, D = 2, 8, 64\n",
    "# for N in [256, 512, 1024, 2048]:\n",
    "#     q = torch.randn(B, H, N, D, device=\"cuda\", dtype=torch.bfloat16)\n",
    "#     k = torch.randn_like(q)\n",
    "#     v = torch.randn_like(q)\n",
    "\n",
    "#     def run_vanilla():\n",
    "#         return VanillaAttention.apply(q, k, v, True)\n",
    "\n",
    "#     def run_flash():\n",
    "#         return FlashAttentionPyTorchImpl.apply(q, k, v, True, 128)\n",
    "\n",
    "#     tv = bench(run_vanilla)\n",
    "#     tf = bench(run_flash)\n",
    "#     print(f\"N={N:4d}  vanilla={tv * 1e3:7.2f} ms  flash-like={tf * 1e3:7.2f} ms  speedup={tv / tf:5.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef6fdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtl\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pad_to_multiple\u001b[39m(x: torch.Tensor, size: \u001b[38;5;28mint\u001b[39m, dim: \u001b[38;5;28mint\u001b[39m, value: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'triton'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725661a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
