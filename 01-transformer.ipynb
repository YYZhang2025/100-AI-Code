{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "d19bd22f"
   },
   "source": [
    "This is the corresponding code for the [01: Attention is All You Need](https://yyzhang2025.github.io/100-AI-Papers/posts/01-attention.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>\n",
    "\n",
    "- [Helper Functions](#toc1_1_)\n",
    "- [Transformer Model Implementation](#toc1_2_)\n",
    "  - [Embedding](#toc1_2_1_)\n",
    "  - [Layer Normalization](#toc1_2_2_)\n",
    "  - [Feedforward Neural Network](#toc1_2_3_)\n",
    "  - [Attention](#toc1_2_4_)\n",
    "  - [Encoder Block](#toc1_2_5_)\n",
    "  - [Decoder Block](#toc1_2_6_)\n",
    "  - [Encoder & Decoder](#toc1_2_7_)\n",
    "  - [Transformer Model](#toc1_2_8_)\n",
    "  - [Dummy Test](#toc1_2_9_)\n",
    "- [Dataset Preparation](#toc1_3_)\n",
    "  - [Train BPE Tokenizer](#toc1_3_1_)\n",
    "  - [Define Dataset](#toc1_3_2_)\n",
    "- [Training Procedure](#toc1_4_)\n",
    "  - [Loss Function with Label Smoothing](#toc1_4_1_)\n",
    "  - [Adam Optimizer](#toc1_4_2_)\n",
    "- [Training Loop](#toc1_5_)\n",
    "- [Appendix](#toc1_6_)\n",
    "  - [Visualize Learning Rate](#toc1_6_1_)\n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1753180752931,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "NadghT5GrPa9"
   },
   "outputs": [],
   "source": [
    "# ! pip uninstall -y datasets\n",
    "# ! pip install datasets==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Helper Functions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def expand_tensor(tensor, target_dim, head=True):\n",
    "    \"\"\"\n",
    "    Expands the tensor's dimensions until it reaches the target number of dimensions.\n",
    "    tensor (torch.Tensor): The input tensor to expand.\n",
    "    target_dim (int): Desired number of dimensions.\n",
    "    head (bool): If True, adds dimensions to the front;\n",
    "                    if False, adds to the tail (end).\n",
    "    \"\"\"\n",
    "\n",
    "    while tensor.ndim < target_dim:\n",
    "        tensor = tensor.unsqueeze(0) if head else tensor.unsqueeze(-1)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def create_causal_mask(seq_len_q, seq_len_k=None):\n",
    "    \"\"\"\n",
    "    Create a causal mask for the attention mechanism.\n",
    "    seq_len_q: Length of the query sequence\n",
    "    seq_len_k: Length of the key sequence\n",
    "    \"\"\"\n",
    "    if seq_len_k is None:\n",
    "        seq_len_k = seq_len_q\n",
    "\n",
    "    mask = torch.triu(torch.ones(seq_len_q, seq_len_k), diagonal=1).bool()\n",
    "\n",
    "    return mask.unsqueeze(0)  # (1, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "def create_padding_mask(x, padding_idx=0):\n",
    "    \"\"\"\n",
    "    Create a padding mask for the attention mechanism.\n",
    "    seq_len: Length of the sequence\n",
    "    padding_idx: Index used for padding (default is 0)\n",
    "    \"\"\"\n",
    "    mask = x == padding_idx  # (seq_len)\n",
    "    mask = expand_tensor(mask, 3)  # (1, 1, seq_len)\n",
    "\n",
    "    return mask  # (1, 1, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    src_vocab_size: int = 16000\n",
    "    tgt_vocab_size: int = 16000\n",
    "    max_seq: int = 128\n",
    "\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 2048\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    eps: float = 1e-6  # for Layer Normalization\n",
    "\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Transformer Model Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Embedding](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, is_tgt: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        if is_tgt:\n",
    "            self.embedding = nn.Embedding(config.tgt_vocab_size, config.d_model)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.src_vocab_size, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        pos_index = torch.arange(config.max_seq).unsqueeze(1)  # (max_seq, 1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, config.d_model, 2) / config.d_model * -math.log(10000.0)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(config.max_seq, config.d_model)  # (max_seq, d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos_index * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos_index * div_term)\n",
    "\n",
    "        pe = expand_tensor(pe, 3)  # (1, max_seq, d_model)\n",
    "\n",
    "        pe.requires_grad = False\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:, :seq_len, :]  # (1, seq_len, d_model)\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, is_tgt: bool = False):\n",
    "        super().__init__()\n",
    "        self.word_embedding = WordEmbedding(config, is_tgt)\n",
    "        self.positional_embedding = PositionalEmbedding(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        word_emb = self.word_embedding(x)\n",
    "        pos_emb = self.positional_embedding(word_emb)\n",
    "        return word_emb + pos_emb  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Layer Normalization](#toc0_)\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = config.eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(config.d_model))  # (d_model,)\n",
    "        self.beta = nn.Parameter(torch.zeros(config.d_model))  # (d_model,)\n",
    "\n",
    "    def _compute_mean_std(self, x):\n",
    "        \"\"\"\n",
    "        Compute mean and standard deviation for the input tensor x\n",
    "        On the last dimension (features)\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            mean: (batch_size, seq_len, 1)\n",
    "            std: (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # Numerail Stability\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        std = torch.sqrt(var + self.eps)  # manual epsilon for extra safety\n",
    "        return mean, std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, std = self._compute_mean_std(x)\n",
    "        normalized_x = (x - mean) / std  # Avoid division by zero\n",
    "\n",
    "        return normalized_x * self.gamma + self.beta  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Feedforward Neural Network](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.Linear(config.d_model, config.d_ff, bias=True)\n",
    "        self.ln2 = nn.Linear(config.d_ff, config.d_model, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(x))  # Apply ReLU activation\n",
    "        x = self.ln2(x)  # Linear transformation\n",
    "        return x  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[Attention](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    q: (batch_size, num_heads, seq_len_q, d_k)\n",
    "    k: (batch_size, num_heads, seq_len_k, d_k)\n",
    "    v: (batch_size, num_heads, seq_len_v, d_v)\n",
    "    mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        q.ndim == k.ndim == v.ndim == 4\n",
    "    ), \"Query, Key, Value must has form (batch_size, num_heads, seq_len, d)\"\n",
    "    assert (\n",
    "        q.shape[-1] == k.shape[-1]\n",
    "    ), \"Query, Key must have feature dimension, which mean d_k == d_q\"\n",
    "\n",
    "    d_k = k.shape[-1]\n",
    "    scores = einops.einsum(\n",
    "        q,\n",
    "        k,\n",
    "        \"batch heads seq_len_q d_k, batch heads seq_len_k d_k -> batch heads seq_len_q seq_len_k\",\n",
    "    )\n",
    "\n",
    "    scores = scores / math.sqrt(d_k)  # Scale the scores\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))  # Apply mask if provided\n",
    "\n",
    "    scores = F.softmax(scores, dim=-1)  # Apply softmax to get attention weights\n",
    "\n",
    "    output = einops.einsum(\n",
    "        scores,\n",
    "        v,\n",
    "        \"batch heads seq_len_q seq_len_k, batch heads seq_len_k d_v -> batch heads seq_len_q d_v\",\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1753180752995,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "fe877cbb"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            config.d_model % config.num_heads == 0\n",
    "        ), \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.d_k = config.d_model // config.num_heads  # Dimension of each head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(\n",
    "            config.d_model, config.d_model * 3, bias=True\n",
    "        )  # (d_model, d_model * 3)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: einops.rearrange(\n",
    "                t,\n",
    "                \"batch seq_len (heads d_k) -> batch heads seq_len d_k\",\n",
    "                heads=self.num_heads,\n",
    "            ),\n",
    "            self.qkv_proj(x).chunk(3, dim=-1),\n",
    "        )  # (batch, num_heads, seq_len, d_k)\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Rearrange back to (batch_size, seq_len, d_model)\n",
    "        attn_output = einops.rearrange(\n",
    "            attn_output,\n",
    "            \"batch heads seq_len d_v -> batch seq_len (heads d_v)\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(attn_output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        return output  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            config.d_model % config.num_heads == 0\n",
    "        ), \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.d_k = config.d_model // config.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.d_model, config.d_model, bias=True\n",
    "        )  # (d_model, d_model)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: (batch_size, seq_len_q, d_model)\n",
    "        key: (batch_size, seq_len_k, d_model)\n",
    "        value: (batch_size, seq_len_v, d_model)\n",
    "        mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        q = einops.rearrange(\n",
    "            self.q_proj(query),\n",
    "            \"batch seq_len_q (heads d_k) -> batch heads seq_len_q d_k\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "\n",
    "        k = einops.rearrange(\n",
    "            self.k_proj(key),\n",
    "            \"batch seq_len_k (heads d_k) -> batch heads seq_len_k d_k\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "\n",
    "        v = einops.rearrange(\n",
    "            self.v_proj(value),\n",
    "            \"batch seq_len_v (heads d_v) -> batch heads seq_len_v d_v\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Rearrange back to (batch_size, seq_len_q, d_model)\n",
    "        attn_output = einops.rearrange(\n",
    "            attn_output,\n",
    "            \"batch heads seq_len_q d_v -> batch seq_len_q (heads d_v)\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "\n",
    "        return self.out_proj(attn_output)  # (batch_size, seq_len_q, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_5_'></a>[Encoder Block](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln1 = LayerNormalization(config)\n",
    "        self.ln2 = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        out = self.self_attn(x, src_mask)  # (batch_size, seq_len, d_model)\n",
    "        out = self.ln1(out + x)  # Add & Norm\n",
    "        out = self.ffn(out)  # (batch_size, seq_len, d_model\n",
    "        out = self.ln2(out + x)  # Add & Norm\n",
    "\n",
    "        return out  # (batch_size, seq_len, d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_6_'></a>[Decoder Block](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(config)\n",
    "        self.cross_attn = CrossAttention(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln1 = LayerNormalization(config)\n",
    "        self.ln2 = LayerNormalization(config)\n",
    "        self.ln3 = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        out = self.self_attn(x, tgt_mask)  # Self-attention\n",
    "        out = self.ln1(out + x)  # Add & Norm\n",
    "\n",
    "        out = self.cross_attn(out, enc_output, enc_output, src_mask)  # Cross-attention\n",
    "        out = self.ln2(out + x)  # Add & Norm\n",
    "\n",
    "        out = self.ffn(out)  # Feedforward\n",
    "        out = self.ln3(out + x)  # Add & Norm\n",
    "\n",
    "        return out  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_7_'></a>[Encoder & Decoder](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(config)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "        self.ln = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = self.ln(x)  # Final Layer Normalization\n",
    "        return x  # (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(config, is_tgt=True)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "        self.ln = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        enc_output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        x = self.ln(x)  # Final Layer Normalization\n",
    "        return x  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_8_'></a>[Transformer Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n",
    "\n",
    "        self._tie_weight()\n",
    "\n",
    "    def _tie_weight(self):\n",
    "        \"\"\"\n",
    "        Tie the weights of the output layer with the embedding layer.\n",
    "        This is a common practice in Transformer models to reduce the number of parameters.\n",
    "        \"\"\"\n",
    "        self.output_layer.weight = (\n",
    "            self.decoder.embedding.word_embedding.embedding.weight\n",
    "        )\n",
    "\n",
    "    def encode(self, x, src_mask=None):\n",
    "        enc_output = self.encoder(x, src_mask)\n",
    "        return enc_output\n",
    "\n",
    "    def decode(self, enc_output, tgt, src_mask, tgt_mask):\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        return dec_output\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = expand_tensor(src, 2)  # (B, Src_Seq_len)\n",
    "        tgt = expand_tensor(tgt, 2)  # (B, Tgt_Seq_len)\n",
    "\n",
    "        enc_output = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(enc_output, tgt, src_mask, tgt_mask)\n",
    "        logits = self.output_layer(dec_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_9_'></a>[Dummy Test](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig()\n",
    "transformer = Transformer(config).to(DEVICE)\n",
    "\n",
    "src = torch.randint(0, config.src_vocab_size, (32, 10)).to(\n",
    "    DEVICE\n",
    ")  # (batch_size, src_seq_len\n",
    "tgt = torch.randint(0, config.tgt_vocab_size, (32, 15)).to(\n",
    "    DEVICE\n",
    ")  # (batch_size, tgt_seq_len\n",
    "\n",
    "causal_mask = create_causal_mask(tgt.size(1), tgt.size(1))\n",
    "causal_mask = causal_mask.to(DEVICE)\n",
    "\n",
    "output = transformer(src, tgt, tgt_mask=causal_mask)\n",
    "\n",
    "assert output.shape == (*tgt.size(), config.tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Dataset Preparation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from tokenizers.normalizers import Sequence, NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753180753706,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "TWiVbweAdc86"
   },
   "outputs": [],
   "source": [
    "def save_dataset(dataset, file_prefix, src=\"en\", tgt=\"zh\", debug=True):\n",
    "    assert file_prefix is not None, \"file_prefix must be set\"\n",
    "\n",
    "    if debug:\n",
    "        dataset = dataset[: len(dataset) // 100]\n",
    "\n",
    "    src_file_name = file_prefix + \"_src\" + \".txt\"\n",
    "    tgt_file_name = file_prefix + \"_tgt\" + \".txt\"\n",
    "\n",
    "    if os.path.exists(src_file_name) and os.path.exists(tgt_file_name):\n",
    "        print(f\"{src_file_name}, {tgt_file_name} already exists\")\n",
    "        return\n",
    "\n",
    "    with open(src_file_name, \"w\", encoding=\"utf-8\") as src_f, open(\n",
    "        tgt_file_name, \"w\", encoding=\"utf-8\"\n",
    "    ) as tgt_f:\n",
    "        for example in dataset:\n",
    "            src_f.write(example[src].strip() + \"\\n\")\n",
    "            tgt_f.write(example[tgt].strip() + \"\\n\")\n",
    "\n",
    "    print(f\"{src_file_name}, {tgt_file_name} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"iwslt2017\",\n",
    "#     \"iwslt2017-en-zh\",\n",
    "#     download_mode=\"force_redownload\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# train_dataset = dataset['train']['translation']\n",
    "# test_dataset = dataset['test']['translation']\n",
    "\n",
    "# save_dataset(train_dataset, file_prefix='train')\n",
    "# save_dataset(test_dataset, file_prefix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Train BPE Tokenizer](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_train_bpe_tokenizer(vocab_size, ln, save_name=\"bpe.json\"):\n",
    "    save_path = ln + \"_\" + save_name\n",
    "\n",
    "    ### HARD Coded\n",
    "    if ln == \"zh\":\n",
    "        corpus_file = \"train_tgt.txt\"\n",
    "    elif ln == \"en\":\n",
    "        corpus_file = \"train_src.txt\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading tokenizer from {save_path}\")\n",
    "        tokenizer = Tokenizer.from_file(save_path)\n",
    "        return tokenizer\n",
    "    else:\n",
    "        tokenizer = Tokenizer(models.BPE())\n",
    "        tokenizer.normalizer = NFKC()\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
    "        )\n",
    "\n",
    "        tokenizer.train([corpus_file], trainer)\n",
    "        tokenizer.save(save_path)\n",
    "        print(f\"Saved tokenizer to {save_path}\")\n",
    "\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Define Dataset](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_prefix,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        seq_len=128,\n",
    "        is_debug=True,\n",
    "    ):\n",
    "\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.seq_len = seq_len\n",
    "        self.is_debug = is_debug\n",
    "\n",
    "        self.pad_id_src = tokenizer_src.token_to_id(\"<pad>\") or 0\n",
    "        self.pad_id_tgt = tokenizer_tgt.token_to_id(\"<pad>\") or 0\n",
    "\n",
    "        self.sos_id = tokenizer_tgt.token_to_id(\"<s>\") or 1\n",
    "        self.eos_id = tokenizer_tgt.token_to_id(\"</s>\") or 2\n",
    "\n",
    "        self.source_texts, self.target_texts = self._load_text_pairs(file_prefix)\n",
    "\n",
    "    def _load_text_pairs(self, file_prefix):\n",
    "        src_path = file_prefix + \"_src.txt\"\n",
    "        tgt_path = file_prefix + \"_tgt.txt\"\n",
    "\n",
    "        assert os.path.exists(src_path), f\"Source file not found: {src_path}\"\n",
    "        assert os.path.exists(tgt_path), f\"Target file not found: {tgt_path}\"\n",
    "\n",
    "        with open(src_path, encoding=\"utf-8\") as f_src, open(\n",
    "            tgt_path, encoding=\"utf-8\"\n",
    "        ) as f_tgt:\n",
    "            src_lines = [line.strip() for line in f_src if line.strip()]\n",
    "            tgt_lines = [line.strip() for line in f_tgt if line.strip()]\n",
    "\n",
    "        assert len(src_lines) == len(\n",
    "            tgt_lines\n",
    "        ), \"Mismatched number of lines in source and target files\"\n",
    "        if self.is_debug:\n",
    "            src_lines = src_lines[:1000]\n",
    "            tgt_lines = tgt_lines[:1000]\n",
    "        return src_lines, tgt_lines\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.source_texts[idx]\n",
    "        tgt_text = self.target_texts[idx]\n",
    "\n",
    "        src_ids = self._process_source(src_text)\n",
    "        tgt_ids, decoder_ids = self._process_target(tgt_text)\n",
    "\n",
    "        assert decoder_ids.size(0) == self.seq_len\n",
    "        assert tgt_ids.size(0) == self.seq_len\n",
    "        assert src_ids.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input_ids\": src_ids,\n",
    "            \"decoder_input_ids\": decoder_ids,\n",
    "            \"labels\": tgt_ids,\n",
    "            \"encoder_mask\": create_padding_mask(src_ids, self.pad_id_src),\n",
    "            \"decoder_mask\": create_causal_mask(decoder_ids.size(0), decoder_ids.size(0))\n",
    "            & create_padding_mask(decoder_ids, self.pad_id_tgt),\n",
    "        }\n",
    "\n",
    "    def _process_source(self, text: str) -> torch.Tensor:\n",
    "        token_ids = self.tokenizer_src.encode(text).ids\n",
    "        token_ids = self._pad_or_truncate(token_ids, self.seq_len, self.pad_id_src)\n",
    "        return torch.tensor(token_ids, dtype=torch.int64)\n",
    "\n",
    "    def _process_target(self, text: str):\n",
    "        token_ids = self.tokenizer_tgt.encode(text).ids\n",
    "\n",
    "        tgt_ids = token_ids[: self.seq_len - 1] + [self.eos_id]\n",
    "        tgt_ids = self._pad_or_truncate(tgt_ids, self.seq_len, self.pad_id_tgt)\n",
    "\n",
    "        decoder_ids = [self.sos_id] + token_ids[: self.seq_len - 1]\n",
    "        decoder_ids = self._pad_or_truncate(decoder_ids, self.seq_len, self.pad_id_tgt)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(tgt_ids, dtype=torch.int64),\n",
    "            torch.tensor(decoder_ids, dtype=torch.int64),\n",
    "        )\n",
    "\n",
    "    def _pad_or_truncate(self, ids, length, pad_id):\n",
    "        return ids[:length] + [pad_id] * (length - len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Training Procedure](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Loss Function with Label Smoothing](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Adam Optimizer](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{m_{t,i}}{\\sqrt{v_{t,i}}+\\epsilon}$$\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{m}*{t,i} &= \\frac{m*{t,i}}{1-\\beta*1^t} \\\\\n",
    "\\hat{v}_{t,i} &= \\frac{v\\_{t,i}}{1-\\beta_2^t}\n",
    "\\end{align_}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1753180762172,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "1dc1851c"
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, model_params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.model_params = list(model_params)\n",
    "        self.lr = lr\n",
    "        self.beta_1, self.beta_2 = betas\n",
    "        self.eps = eps\n",
    "        self.avg_grads = [torch.zeros_like(p) for p in self.model_params]\n",
    "        self.avg_squares = [torch.zeros_like(p) for p in self.model_params]\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.model_params:\n",
    "            p.grad = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        self.n_steps += 1  # increment ONCE per step\n",
    "\n",
    "        for p, m, v in zip(self.model_params, self.avg_grads, self.avg_squares):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            # Update moving averages\n",
    "            m.mul_(self.beta_1).add_(p.grad, alpha=1 - self.beta_1)\n",
    "            v.mul_(self.beta_2).addcmul_(p.grad, p.grad, value=1 - self.beta_2)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = m / (1 - self.beta_1**self.n_steps)\n",
    "            v_hat = v / (1 - self.beta_2**self.n_steps)\n",
    "\n",
    "            # Parameter update\n",
    "            p.addcdiv_(m_hat, v_hat.sqrt() + self.eps, value=-self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1753180762173,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "lLPWv9Z5mQZr"
   },
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, d_model: int, warmup_steps: int = 4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "\n",
    "    def _compute_lr(self, step: int) -> float:\n",
    "        scale = self.d_model**-0.5\n",
    "        return scale * min(step**-0.5, step * self.warmup_steps**-1.5)\n",
    "\n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        new_lr = self._compute_lr(self.step_num)\n",
    "        self.optimizer.lr = new_lr  # update custom optimizer’s lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, smoothing=0.1, ignore_index: int = 0):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.vocab_size = config.tgt_vocab_size\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eps = smoothing / (self.vocab_size - 1)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: (batch_size, seq_len, vocab_size)\n",
    "        target: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits.view(-1, self.vocab_size)\n",
    "\n",
    "        if target.ndim == 2:\n",
    "            target = target.view(-1)\n",
    "\n",
    "        # Mask padding tokens\n",
    "        mask = target != self.ignore_index\n",
    "        target = target[mask]\n",
    "        logits = logits[mask]\n",
    "\n",
    "        # Compute log-probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Create smoothed one-hot labels\n",
    "        true_dist = torch.full_like(log_probs, self.eps)\n",
    "        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "\n",
    "        loss = F.kl_div(log_probs, true_dist, reduction=\"batchmean\")\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Training Loop](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    data,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    encoder_input_ids = data[\"encoder_input_ids\"].to(DEVICE)\n",
    "    decoder_input_ids = data[\"decoder_input_ids\"].to(DEVICE)\n",
    "    enc_mask = data[\"encoder_mask\"].to(DEVICE)\n",
    "    dec_mask = data[\"decoder_mask\"].to(DEVICE)\n",
    "\n",
    "    labels = data[\"labels\"].to(DEVICE)\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(\n",
    "        encoder_input_ids, decoder_input_ids, src_mask=enc_mask, tgt_mask=dec_mask\n",
    "    )\n",
    "\n",
    "    # Compute loss # Debugging line\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "    # Clear all the graidnet information\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()  # Return the loss value for logging\n",
    "\n",
    "\n",
    "def train(model, dataset, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(dataset, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            loss = train_step(model, optimizer, criterion, batch)\n",
    "            total_loss += loss\n",
    "            losses.append(loss)\n",
    "\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1753180762244,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "aaIxiYxJrkVY"
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def init_transformer(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        init.xavier_uniform_(module.weight)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551,
     "referenced_widgets": [
      "8670ed9b40dd4a48a490a54491c1353b",
      "c7f649aa6fae46f6bfdaecfa2a163409",
      "f8ffb82188c440a592339a481a2a2ffe",
      "58b78c43b70a416a9fb25d0a20930da1",
      "dfdc15497db7465eaebdd816370562dc",
      "e1a95da5adbf42c08827f88ad9fb57f3",
      "16af04aa132f4a7794fea002a7eaaa23",
      "ad54706a352d4c729042d46536618184",
      "3c757aa46f0a463caf96230048e49775",
      "67ba5f73b32642338a20e0220ce9912e",
      "bd0d88d763a74ef282d842eb19c963ed",
      "bda9d6a128ab4642ac988f0d463bffc2",
      "fc3f498ac8874d3bb5126a46bc255fd0",
      "bc5394b60dc44adbbb4b48da2b726d69",
      "0e4bd9e59f2d4c2aad77b8b8725a300b",
      "898523b31cfe4538a8b20cb4882bca18",
      "ca379f3929e547469a3d8ea7092af283",
      "a1cf8e43e81e494b8163e511004ee17a",
      "6db27ff6e81c44df8dac0718636cc79f",
      "d66d671e340044b0a0bf4890fbb1b23b",
      "f76bac968e22464ab10684357dfb62dc",
      "eb746225a670441fa9309f3cf8c4fd6c",
      "d3f27296d5274dfca3e54f7719e29bdc",
      "49b36dcba7a64090bad3706391d6f2dd",
      "b6ed02420ea24767bf298eec27713b0b",
      "884fae0d6b6243a9a59060aeebf8b9c6",
      "de04a95f1f13428eae3f4613881213e4",
      "fc85c0b41d1046c68d853328f3c58359",
      "1525834bbbe2458a8f35fcc21b7f92c2",
      "7e8088356d664b5c85b199f4a00d94a8",
      "e9847ce4d98b45ecb53f591867ac394c",
      "7a47fba552bd41cbba9b0641055fb0ec",
      "6caac87bd3a0478cabfb0cd1b57cfcff",
      "2c62be3c258b488e9eb4463bc82d3a15",
      "9f60ff5649f24dfe8ed1bc410b713c54",
      "79f14f9d0cbe48fc872b26617d411f3e",
      "6f908cb9f4ef4ec9a2fefcf012dd1ddf",
      "e57e43c29e6541b18eaf4375a1c42898",
      "4fa730d1899a4bbb9b854e01a960e6b4",
      "2d5b4a9d468f4f33bcc74bf9a62421fe",
      "0c432ef708174b88b938969ebb226a18",
      "39601dd6f1794fee91661815fa452b36",
      "ec78bc7e135f4216b3591e1c4c596299",
      "8c2b6abfd15646bb9b9fe74d664938ec",
      "c4839e566b044038bff873fafcb31c74",
      "2fa2b3ed97cb403f832845adf541f24e",
      "8444603fd02a46c8a54ba2439b042efb",
      "3255cab581ed40d28c8da5c6b0e0e384",
      "5f941f923cdb4576b36ba507f119cb85",
      "07fbb95d0cbf45ba932eb48a39470c31",
      "6a660425b373476e8bf55909d829655d",
      "d51cc7c77a5e4e06acfd322ef4f83489",
      "813897992e2b4e93afeb6b58b1bd9541",
      "ef5b3a812f524db89e3378b6dffccb2e",
      "d4b4097fbeb5476e8e8ca9f8d324ebaf",
      "0a8a8cf72b54461e81b2030c9adb61be",
      "5227e86d23274133be2bc9892d2016c4",
      "be9934d234724e33b5475c8f4c079821",
      "daf46c5870244077b33bdb8817d2ea36",
      "cccb2584e675454b903869dbc7046902",
      "66bd6545a8df4153890224c6c22c94cc",
      "1209ab5bd97a45abb42a4a9b7ab4cdff",
      "61a9202118b44bc8ba3f4b028711c5bf",
      "d497bf9c1e734550a36e339bbd6f2516",
      "9b7b2f68540c4bf5b216cfc12afc8cd8",
      "df4982e802a04c2984f0cf3a030bedc1",
      "4ff4c270ceee4195ab1d270812099ea8",
      "eb44b51947934d95b1965a1b32a21785",
      "e9b49a62b3094a159e561253590b9c3b",
      "961e15e116504ff0a5b0b92ced857d30",
      "1e8f94f0c5e84688ac5d86e115acd986",
      "d508cf2850bd437f8e3159033ac158e9",
      "8f2d3e1d288442318bcd3c0d0e8f355f",
      "466c6f83bfa740dd972303c6498835cf",
      "bf74fcae0522424f9acbbe950170c037",
      "95e49d6f1b27463881f94c42f70d43b2",
      "5db489c5c97f435089bceb7edf94e4ae",
      "b8e48376ca29404f88bd248909a7229c",
      "cd66253ac6c1462d9da1f1adb1fd7399",
      "7ac2656997ca434f878e54d7802d8050",
      "838e868f14f5491b97f39280eb58a9c6",
      "330094e59c4d451c823bbc9b43aee1f9",
      "786118ce64ad4785af3d96bb8f99b8f1",
      "042e37d56d484023a195699f7c0e1bda",
      "469e60efe1fc416592496c99c78f25f1",
      "bc025564cf254f93ad24db83cbffc1c7",
      "186b75f0badc4ba29a89d0b052f496e7",
      "c857f111586e40708d7c456f83e0ad50",
      "23467376c16b4ab7ae8fcc713ad31ed5",
      "3a1726b371a2446284a4606720b5542f",
      "61b84122446944aca05020ba16a6d3c6",
      "8b815b33a0c440ba869608ff6a4eae28",
      "fadcacd11c7140a1b9f1e1e9f680acfa",
      "5b629e107e6a4003bb03cc66215d7916",
      "57a792cc5ca74e1f80433dbb6a429d64",
      "c4cd362a41124f58992d91e401023833",
      "0afca80667f047c5b64f821e9700e73d",
      "f97d3f7f552046d3838789b1e4d62d06",
      "a5d6e1d4b173475cb430fe9c7057da90",
      "cebc3a0c1b36461aaa11baede8602454",
      "481d1244c1544853bc4a717bd20c39a2",
      "a3573e1612f2439c858d3eee3b7b5208",
      "8ef844bf57c14fbc8fbd4467337cddda",
      "c42cda8ce0d142589b81ddaad1bf2f11",
      "39c00901615449128c55b16727c8904c",
      "eab0858075e1461ca2628ee0a8270cf7",
      "fda14247a9bc4cd69ee276b1708cd146",
      "1072e9e9199742b9a27670f0898fee64",
      "096002b3c6f84498b846816ff23b0b1f",
      "6f09cca5b0d046d68b0c370f0d582b2d"
     ]
    },
    "executionInfo": {
     "elapsed": 138273,
     "status": "ok",
     "timestamp": 1753181180624,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "cf82ff10",
    "outputId": "f82de64d-0ed1-464c-d2b5-2de917e1fbb6"
   },
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "config = ModelConfig()\n",
    "\n",
    "model = Transformer(config)\n",
    "model.apply(init_transformer)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "src_tokenizer = load_or_train_bpe_tokenizer(vocab_size=config.src_vocab_size, ln=\"en\")\n",
    "tgt_tokenizer = load_or_train_bpe_tokenizer(vocab_size=config.tgt_vocab_size, ln=\"zh\")\n",
    "\n",
    "translation_dataset = TranslationDataset(\n",
    "    file_prefix=\"train\",\n",
    "    tokenizer_src=src_tokenizer,\n",
    "    tokenizer_tgt=tgt_tokenizer,\n",
    "    seq_len=config.max_seq,\n",
    "    is_debug=True,\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,  # shuffle for training\n",
    "    num_workers=2,  # or 0 if debugging or on Windows\n",
    "    pin_memory=True,  # for faster GPU transfer (optional)\n",
    "    drop_last=False,  # whether to drop the last incomplete batch\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = LRScheduler(optimizer, d_model=config.d_model, warmup_steps=40)\n",
    "criterion = LabelSmoothing(config, ignore_index=0).to(DEVICE)\n",
    "\n",
    "losses = train(model, train_dataloader, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1753181180848,
     "user": {
      "displayName": "YUYANG ZHANG",
      "userId": "13229668227307969325"
     },
     "user_tz": -480
    },
    "id": "Cx6TFZC5sz6W",
    "outputId": "e380a306-d8eb-4325-b65d-db6e4b1772e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def smooth_curve(values, window_size=100):\n",
    "    if len(values) < window_size:\n",
    "        return values  # skip smoothing if not enough data\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    return np.convolve(values, kernel, mode=\"valid\")\n",
    "\n",
    "\n",
    "def plot_loss_curve(losses, window_size=100):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses, label=\"Step Loss\")\n",
    "    smoothed = smooth_curve(losses, window_size)\n",
    "    plt.plot(range(len(smoothed)), smoothed, label=\"Smoothed Loss\", color=\"orange\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Per Step\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss_curve(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = \"Hi, how are you, This is test\"\n",
    "# pad engligh\n",
    "src_ids = src_tokenizer.encode(english).ids + [src_tokenizer.token_to_id(\"<pad>\")] * (\n",
    "    128 - len(src_tokenizer.encode(english).ids)\n",
    ")\n",
    "src_ids = torch.tensor(src_ids, dtype=torch.int64)\n",
    "src_mask = create_padding_mask(src_ids, padding_idx=0)\n",
    "\n",
    "src_ids = expand_tensor(src_ids, 2)\n",
    "\n",
    "decoder_input = (\n",
    "    torch.tensor(tgt_tokenizer.token_to_id(\"<s>\"), dtype=torch.int64)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    ")\n",
    "\n",
    "src_ids = src_ids.to(DEVICE)\n",
    "decoder_input = decoder_input.to(DEVICE)\n",
    "src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "while decoder_input.size(1) < 128:\n",
    "    decocer_mask = create_causal_mask(decoder_input.size(0), decoder_input.size(0))\n",
    "\n",
    "    decocer_mask = decocer_mask.to(DEVICE)\n",
    "    out = model(src_ids, decoder_input, src_mask=src_mask, tgt_mask=decocer_mask)\n",
    "\n",
    "    _, next_word = torch.max(out[:, -1], dim=-1)\n",
    "\n",
    "    decoder_input = torch.cat(\n",
    "        [\n",
    "            decoder_input,\n",
    "            torch.empty(1, 1).type_as(src_ids).fill_(next_word.item()).to(DEVICE),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    if next_word == tgt_tokenizer.token_to_id(\"</s>\"):\n",
    "        break\n",
    "\n",
    "output = tgt_tokenizer.decode(decoder_input[0].tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Appendix](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[Visualize Learning Rate](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_lr_schedule(scheduler, num_steps=10000):\n",
    "    lrs = []\n",
    "\n",
    "    # Save the original step number and restore it later\n",
    "    original_step = scheduler.step_num\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        scheduler.step()\n",
    "        lrs.append(scheduler.optimizer.lr)\n",
    "\n",
    "    # Restore step counter if needed\n",
    "    scheduler.step_num = original_step\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(lrs)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(f\"Transformer LR Schedule (warmup={scheduler.warmup_steps})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Dummy optimizer with `.lr` field\n",
    "class DummyOptimizer:\n",
    "    def __init__(self):\n",
    "        self.lr = 0.0\n",
    "\n",
    "\n",
    "# Create scheduler\n",
    "optimizer = DummyOptimizer()\n",
    "scheduler = LRScheduler(optimizer, d_model=512, warmup_steps=4000)\n",
    "\n",
    "# Plot\n",
    "plot_lr_schedule(scheduler, num_steps=20000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rOdJUnLIUBSZ",
    "d96c187f",
    "MB9e2iJHaG_w"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
