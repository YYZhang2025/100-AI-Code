{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19bd22f",
   "metadata": {},
   "source": [
    "This is the corresponding code for the [01: Attention is All You Need](https://yyzhang2025.github.io/100-AI-Papers/posts/01-attention.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b84174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    src_vocab_size: int = 10000\n",
    "    tgt_vocab_size: int = 10000\n",
    "    max_seq: int = 512\n",
    "\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 2048\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    eps: float = 1e-6  # for Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c187f",
   "metadata": {},
   "source": [
    "## Transformer Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250e4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, is_tgt: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        if is_tgt:\n",
    "            self.embedding = nn.Embedding(config.tgt_vocab_size, config.d_model)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.src_vocab_size, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        pos_index = torch.arange(config.max_seq).unsqueeze(1)  # (max_seq, 1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, config.d_model, 2) * -(math.log(10000.0) / config.d_model)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(config.max_seq, config.d_model)  # (max_seq, d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos_index * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos_index * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_seq, d_model)\n",
    "\n",
    "        pe.requires_grad = False\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:, :seq_len, :]  # (1, seq_len, d_model)\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, is_tgt: bool = False):\n",
    "        super().__init__()\n",
    "        self.word_embedding = WordEmbedding(config, is_tgt)\n",
    "        self.positional_embedding = PositionalEmbedding(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        word_emb = self.word_embedding(x)\n",
    "        pos_emb = self.positional_embedding(word_emb)\n",
    "        return word_emb + pos_emb  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ce00f",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = config.eps\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones(config.d_model))  # (d_model,)\n",
    "        self.beta = nn.Parameter(torch.zeros(config.d_model))  # (d_model,)\n",
    "\n",
    "    def _compute_mean_std(self, x):\n",
    "        \"\"\"\n",
    "        Compute mean and standard deviation for the input tensor x\n",
    "        On the last dimension (features)\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            mean: (batch_size, seq_len, 1)\n",
    "            std: (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return mean, std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, std = self._compute_mean_std(x)\n",
    "\n",
    "        # normalize x: (batch_size, seq_len, d_model)\n",
    "        normalized_x = (x - mean) / (std + self.eps)  # Avoid division by zero\n",
    "\n",
    "        return normalized_x * self.gamma + self.beta  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0c8b7",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf86af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.Linear(config.d_model, config.d_ff, bias=True)\n",
    "        self.ln2 = nn.Linear(config.d_ff, config.d_model, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(x))  # Apply ReLU activation\n",
    "        x = self.ln2(x)  # Linear transformation\n",
    "        return x  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd4429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    q: (batch_size, num_heads, seq_len_q, d_k)\n",
    "    k: (batch_size, num_heads, seq_len_k, d_k)\n",
    "    v: (batch_size, num_heads, seq_len_v, d_v)\n",
    "    mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n",
    "    \"\"\"\n",
    "    d_k = k.shape[-1]\n",
    "\n",
    "    scores = einops.einsum(\n",
    "        \"batch heads seq_len_q d_k, batch heads seq_len_k d_k -> batch heads seq_len_q seq_len_k\",\n",
    "        q,\n",
    "        k,\n",
    "    )\n",
    "\n",
    "    scores = scores / math.sqrt(d_k)  # Scale the scores\n",
    "    scores = F.softmax(scores, dim=-1)  # Apply softmax to get attention weights\n",
    "\n",
    "    if mask:\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))  # Apply mask if provided\n",
    "\n",
    "    output = einops.einsum(\n",
    "        \"batch heads seq_len_q seq_len_k, batch heads seq_len_k d_v -> batch heads seq_len_q d_v\",\n",
    "        scores,\n",
    "        v,\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe877cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            config.d_model % config.num_heads == 0\n",
    "        ), \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = config.d_model // config.num_heads  # Dimension of each head\n",
    "        self.num_heads = config.num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(\n",
    "            config.d_model, config.d_model * 3, bias=True\n",
    "        )  # (d_model, d_model * 3)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: einops.rearrange(\n",
    "                t,\n",
    "                \"batch seq_len (heads d_k) -> batch heads seq_len d_k\",\n",
    "                heads=self.num_heads,\n",
    "            ),\n",
    "            self.qkv_proj(x).chunk(3, dim=-1),\n",
    "        )  # (batch, num_heads, seq_len, d_k)\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Rearrange back to (batch_size, seq_len, d_model)\n",
    "        attn_output = einops.rearrange(\n",
    "            attn_output,\n",
    "            \"batch heads seq_len d_v -> batch seq_len (heads d_v)\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(attn_output)  # (batch_size, seq_len, d_model)\n",
    "        return output  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            config.d_model % config.num_heads == 0\n",
    "        ), \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = config.d_model // config.num_heads  # Dimension of each head\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.d_model, config.d_model, bias=True\n",
    "        )  # (d_model, d_model)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: (batch_size, seq_len_q, d_model)\n",
    "        key: (batch_size, seq_len_k, d_model)\n",
    "        value: (batch_size, seq_len_v, d_model)\n",
    "        mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        q = einops.rearrange(\n",
    "            self.q_proj(query),\n",
    "            \"batch seq_len_q d_model -> batch heads seq_len_q d_k\",\n",
    "            heads=self.num_heads,\n",
    "            d_k=self.d_k,\n",
    "        )\n",
    "\n",
    "        k = einops.rearrange(\n",
    "            self.k_proj(key),\n",
    "            \"batch seq_len_k d_model -> batch heads seq_len_k d_k\",\n",
    "            heads=self.num_heads,\n",
    "            d_k=self.d_k,\n",
    "        )\n",
    "\n",
    "        v = einops.rearrange(\n",
    "            self.v_proj(value),\n",
    "            \"batch seq_len_v d_model -> batch heads seq_len_v d_v\",\n",
    "            heads=self.num_heads,\n",
    "            d_v=self.d_k,  # Assuming d_v == d_k\n",
    "        )\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # Rearrange back to (batch_size, seq_len_q, d_model)\n",
    "        attn_output = einops.rearrange(\n",
    "            attn_output,\n",
    "            \"batch heads seq_len_q d_v -> batch seq_len_q (heads d_v)\",\n",
    "            heads=self.num_heads,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a4f10",
   "metadata": {},
   "source": [
    "### Encoder Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln1 = LayerNormalization(config)\n",
    "        self.ln2 = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.self_attn(x)  # (batch_size, seq_len, d_model)\n",
    "        out = self.ln1(out + x)  # Add & Norm\n",
    "        out = self.ffn(out)  # (batch_size, seq_len, d_model\n",
    "        out = self.ln2(out + x)  # Add & Norm\n",
    "        return out  # (batch_size, seq_len, d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbed466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(config)\n",
    "        self.cross_attn = CrossAttention(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln1 = LayerNormalization(config)\n",
    "        self.ln2 = LayerNormalization(config)\n",
    "        self.ln3 = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x, enc_output, mask=None):\n",
    "        out = self.self_attn(x)  # Self-attention\n",
    "        out = self.ln1(out + x)  # Add & Norm\n",
    "\n",
    "        out = self.cross_attn(out, enc_output, enc_output, mask)  # Cross-attention\n",
    "        out = self.ln2(out + x)  # Add & Norm\n",
    "\n",
    "        out = self.ffn(out)  # Feedforward\n",
    "        out = self.ln3(out + x)  # Add & Norm\n",
    "\n",
    "        return out  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59ed041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(config)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "        self.ln = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln(x)  # Final Layer Normalization\n",
    "        return x  # (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(config, is_tgt=True)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "        self.ln = LayerNormalization(config)\n",
    "\n",
    "    def forward(self, x, enc_output, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        enc_output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, mask)\n",
    "        x = self.ln(x)  # Final Layer Normalization\n",
    "        return x  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, mask=None):\n",
    "        \"\"\"\n",
    "        src: (batch_size, src_seq_len)\n",
    "        tgt: (batch_size, tgt_seq_len)\n",
    "        mask: (batch_size, 1, tgt_seq_len, src_seq_len) or None\n",
    "        \"\"\"\n",
    "        enc_output = self.encoder(src)  # (batch_size, src_seq_len, d_model)\n",
    "        dec_output = self.decoder(\n",
    "            tgt, enc_output, mask\n",
    "        )  # (batch_size, tgt_seq_len, d_model)\n",
    "        output = self.output_layer(\n",
    "            dec_output\n",
    "        )  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        return output  # Final output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adfd61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len_q, seq_len_k):\n",
    "    \"\"\"\n",
    "    Create a causal mask for the attention mechanism.\n",
    "    seq_len_q: Length of the query sequence\n",
    "    seq_len_k: Length of the key sequence\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len_q, seq_len_k), diagonal=1).bool()\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "\n",
    "def create_padding_mask(seq_len, padding_idx=0):\n",
    "    \"\"\"\n",
    "    Create a padding mask for the attention mechanism.\n",
    "    seq_len: Length of the sequence\n",
    "    padding_idx: Index used for padding (default is 0)\n",
    "    \"\"\"\n",
    "    mask = (torch.arange(seq_len).unsqueeze(0) == padding_idx).unsqueeze(\n",
    "        1\n",
    "    )  # (1, 1, seq_len)\n",
    "    return mask  # (1, 1, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959afe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
