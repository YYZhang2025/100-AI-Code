{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Common Components](#toc1_)    \n",
    "  - [Position Encoding](#toc1_1_)    \n",
    "  - [Multi Headed Attention](#toc1_2_)    \n",
    "  - [Feed Forward Network](#toc1_3_)    \n",
    "  - [Encoder Block](#toc1_4_)    \n",
    "- [Text Processing](#toc2_)    \n",
    "  - [Text Tokenizer](#toc2_1_)    \n",
    "  - [Text Encoder](#toc2_2_)    \n",
    "- [Image Processing](#toc3_)    \n",
    "- [CLIP Model](#toc4_)    \n",
    "  - [Modality Projector](#toc4_1_)    \n",
    "  - [Final CLIP Model](#toc4_2_)    \n",
    "  - [Contrastive Loss](#toc4_3_)    \n",
    "  - [Data Sets](#toc4_4_)    \n",
    "- [Zero-Shot Experiment](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "eb66677c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "4564ac1b"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Vision Encoder\n",
    "    vision_image_size:int = 32\n",
    "    vision_patch_size:int = 8\n",
    "    vision_num_channels: int = 1\n",
    "    vision_num_layers: int = 6\n",
    "    vision_num_heads: int = 4\n",
    "    vision_hidden_size: int = 256\n",
    "    vision_ffn_dim: int = 1024\n",
    "\n",
    "    # Text Encoder\n",
    "    lm_vocab_size: int = 256\n",
    "    lm_max_seq_len: int = 32\n",
    "    lm_hidden_size: int = 512\n",
    "    lm_hidden_size: int = 256\n",
    "    lm_num_layers: int = 6\n",
    "    lm_num_heads: int = 4\n",
    "    lm_ffn_dim: int = 1024\n",
    "\n",
    "    # CLIP\n",
    "    clip_hidden_dim: int = 512\n",
    "    clip_loss_temperature: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgIL5cSfVqB7",
    "outputId": "70f37ccc-5da4-4ff5-b483-1ecab00eb91c"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "96c4f060"
   },
   "source": [
    "# <a id='toc1_'></a>[Common Components](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "yYorJV17WcWK"
   },
   "source": [
    "## <a id='toc1_1_'></a>[Position Encoding](#toc0_)\n",
    "For the simplty we both use sin/cos position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "c77a23e8"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config: Config, type: str='lm'):\n",
    "        super().__init__()\n",
    "\n",
    "        if type == 'lm':\n",
    "            seq_len = config.lm_max_seq_len\n",
    "        else:\n",
    "            seq_len =( config.vision_image_size // config.vision_patch_size) ** 2\n",
    "\n",
    "\n",
    "        pos = torch.arange(0, config.lm_max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, config.lm_hidden_size, 2, dtype=torch.float) * -(math.log(10000.0) / config.lm_hidden_size))\n",
    "\n",
    "        pe = torch.zeros(config.lm_max_seq_len, config.lm_hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "8aVZNyLMWht5"
   },
   "source": [
    "## <a id='toc1_2_'></a>[Multi Headed Attention](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "0fb6142e"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        B, S, _ = x.shape\n",
    "        Q, K, V = map(\n",
    "            lambda t: t.view(t.size(0), t.size(1), self.num_heads, self.head_dim).transpose(1, 2),\n",
    "            self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, :, :]\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        #([64, 4, 32, 32]) torch.Size([64, 1, 32, 32])\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, S, -1)\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "yQOchO50WkKO"
   },
   "source": [
    "## <a id='toc1_3_'></a>[Feed Forward Network](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "767cfcd0"
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ffn_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, ffn_dim)\n",
    "        self.linear2 = nn.Linear(ffn_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "dt6xLHaeWnSB"
   },
   "source": [
    "## <a id='toc1_4_'></a>[Encoder Block](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "136dcebb"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,  hidden_dim: int, num_heads: int, ffn_dim: int, is_causal: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.attention = Attention(hidden_dim, num_heads)\n",
    "        self.ffn = FFN(hidden_dim, ffn_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = x + self.attention(self.norm1(x), mask=mask)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "78faf716"
   },
   "source": [
    "# <a id='toc2_'></a>[Text Processing](#toc0_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "8e51Hx_EXdDa"
   },
   "source": [
    "## <a id='toc2_1_'></a>[Text Tokenizer](#toc0_)\n",
    "We just use byte-level tokenizer:\n",
    "- Adds `chr(2)` as `[SOS`] (start of sequence) and `chr(3)` as `[EOS]` (end of sequence)\n",
    "- Encoding: Uses `.encode(\"utf-8\")` to convert characters into bytes, and then wraps that in torch.tensor(...)\n",
    "- Decoding: Converts byte indices back to characters using chr(...), and removes [SOS]/[EOS] during decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "bf7d589c"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text, encode=True, mask = None, max_length=32):\n",
    "    \"\"\"\n",
    "    A simple tokenizer that splits text into tokens.\n",
    "    For demonstration purposes, it just splits by whitespace.\n",
    "    \"\"\"\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3) # add start and end tokens\n",
    "        out = out + \"\".join([chr(0) for _ in range(max_length - len(out))])  # pad to max_length\n",
    "        out = torch.IntTensor(list(out.encode('utf-8')))\n",
    "\n",
    "        mask = torch.ones(len(out.nonzero()), dtype=torch.int32)\n",
    "        mask = torch.cat([mask, torch.zeros(max_length - len(mask), dtype=torch.int32)])\n",
    "    else:\n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "h_DcfGO_Xgm-"
   },
   "source": [
    "## <a id='toc2_2_'></a>[Text Encoder](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "f9654755"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(config.lm_vocab_size, config.lm_hidden_size)\n",
    "        self.positional_embedding = PositionalEmbedding(config, type='lm')\n",
    "        self.transformer = nn.ModuleList([\n",
    "            Encoder(config.lm_hidden_size, config.lm_num_heads, config.lm_ffn_dim, is_causal=True)\n",
    "            for _ in range(config.lm_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.eos_token_id = config.eos_token_id if hasattr(config, 'eos_token_id') else 3\n",
    "\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        \"\"\"\n",
    "        text: Tensor of shape (batch_size, seq_len)\n",
    "        mask: Tensor of shape (batch_size, seq_len) or None\n",
    "        \"\"\"\n",
    "        x = self.embedding(text)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x, mask=mask)\n",
    "\n",
    "        # Extract the [eos] token representation\n",
    "        eos_pos = (text == self.eos_token_id).nonzero(as_tuple=True)[1]\n",
    "        eos_feature = x[torch.arange(x.size(0)), eos_pos, :]\n",
    "\n",
    "        # Normalize the output\n",
    "        eos_feature = F.normalize(eos_feature, dim=-1)\n",
    "\n",
    "        return eos_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "1abb380f"
   },
   "source": [
    "# <a id='toc3_'></a>[Image Processing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "hsTB2bw-X9sJ"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Mean and std of FashionMNIST grayscale images\n",
    "FASHION_MNIST_MEAN = 0.2860\n",
    "FASHION_MNIST_STD = 0.3530\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),                     # Resize to 32x32\n",
    "    transforms.ToTensor(),                           # Convert to tensor, scales [0,255] â†’ [0.0,1.0]\n",
    "    transforms.Normalize(mean=[FASHION_MNIST_MEAN],  # Normalize grayscale images\n",
    "                         std=[FASHION_MNIST_STD]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "874f6ca1"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=config.vision_hidden_size,\n",
    "            kernel_size=config.vision_patch_size,\n",
    "            stride=config.vision_patch_size,\n",
    "            padding='valid'\n",
    "            )\n",
    "        self.positional_embedding = PositionalEmbedding(config, type='vision')\n",
    "\n",
    "        self.transformer = nn.ModuleList([\n",
    "            Encoder(config.vision_hidden_size, config.vision_num_heads, config.vision_ffn_dim, is_causal=False)\n",
    "            for _ in range(config.vision_num_layers)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        # Implement the forward pass for the image encoder\n",
    "        x = self.conv_proj(images)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (batch_size, num_patches, hidden_dim)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "\n",
    "        cls_feature =  x[: , 0, :]\n",
    "        return  cls_feature # Return the representation of the first patch (CLS token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "Q_mu-VnLYP9S"
   },
   "source": [
    "# <a id='toc4_'></a>[CLIP Model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "JtZ6vdUFYRDl"
   },
   "source": [
    "## <a id='toc4_1_'></a>[Modality Projector](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "8b976c6d"
   },
   "outputs": [],
   "source": [
    "class ModalityProjector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"        x: Tensor of shape (batch_size, in_dim)\n",
    "        \"\"\"\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "c7iiB-uEYSoY"
   },
   "source": [
    "## <a id='toc4_2_'></a>[Final CLIP Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "efd469ae"
   },
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.text_encoder = TextEncoder(config)\n",
    "        self.image_encoder = ImageEncoder(config)\n",
    "\n",
    "        self.text_projector = ModalityProjector(config.lm_hidden_size, config.clip_hidden_dim)\n",
    "        self.image_projector = ModalityProjector(config.vision_hidden_size, config.clip_hidden_dim)\n",
    "\n",
    "    def encode_text(self, text, mask = None):\n",
    "        text_features = self.text_encoder(text, mask=mask)\n",
    "        text_features = self.text_projector(text_features)\n",
    "\n",
    "        return text_features\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        image_features = self.image_encoder(images)\n",
    "        image_features = self.image_projector(image_features)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "\n",
    "    def forward(self, text, images, mask=None):\n",
    "        image_features = self.encode_image(images)\n",
    "        text_features = self.encode_text(text, mask)\n",
    "\n",
    "\n",
    "        return text_features, image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "bdX9PpB3YVm6"
   },
   "source": [
    "## <a id='toc4_3_'></a>[Contrastive Loss](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "7eadfcaa"
   },
   "outputs": [],
   "source": [
    "def clip_loss(text_features, image_features, temperature=0.07):\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "    logits = torch.matmul(text_features, image_features.transpose(-2, -1))* (1 / temperature)\n",
    "\n",
    "    labels = torch.arange(text_features.size(0), device=text_features.device)\n",
    "\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.transpose(-2, -1), labels)\n",
    "\n",
    "    return (loss_i + loss_t) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "658a9957"
   },
   "source": [
    "## <a id='toc4_4_'></a>[Data Sets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "58732777"
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, train: bool = True, transform = None ):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        self.dataset = FashionMNIST(root='./data', train=train, download=True)\n",
    "        self.captions = {i: \"a photo of a \" + self.dataset.classes[i] for i in range(len(self.dataset.classes))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "\n",
    "        image = image.convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = self.captions[label]\n",
    "\n",
    "        text, pad_mask = tokenizer(caption, encode=True, max_length=32)\n",
    "        causal_mask = torch.tril(torch.ones((len(text), len(text))), diagonal=0).bool() # (S, S)\n",
    "        causal_mask = causal_mask  # Add head dimensions\n",
    "        if self.train and pad_mask is not None:\n",
    "            pad_mask = pad_mask  # Add head dimensions\n",
    "            mask = causal_mask & pad_mask\n",
    "        else:\n",
    "            mask = causal_mask\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'mask': mask,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "63b228f1"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "train_set = CLIPDataset(train = True, transform=image_transform)\n",
    "test_set = CLIPDataset(train = False, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4c95c90",
    "outputId": "fa4fee05-d243-4dd1-bfb9-6a7c696ec23d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = CLIP(Config())\n",
    "mdoel = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "model.train()\n",
    "best_loss = float('inf')\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    best_loss = float('inf')\n",
    "    epoch_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{epochs}]\", leave=False)\n",
    "\n",
    "    for data in epoch_bar:\n",
    "        img, text, mask = data['image'], data['text'], data['mask']\n",
    "        img = img.to(device)\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        img_features, text_features = model(text, img, mask=mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss = clip_loss(img_features, text_features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_bar.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        if loss.item() <= best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(model.state_dict(), \"best_clip.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "4kMNs_tIeOXw",
    "outputId": "c9e4e9d1-cf20-48f2-dabc-e5f43cc3d6c0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CLIP Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "cede5e72"
   },
   "source": [
    "# <a id='toc5_'></a>[Zero-Shot Experiment](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "3908d3b7"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "ugKtoZA4ecqK",
    "outputId": "df35bcc4-98a7-4ce6-b173-a2d72dda7183"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Load model and tokenizer ----\n",
    "model = CLIP(Config()).to(device)\n",
    "model.load_state_dict(torch.load(\"best_clip.pt\"))\n",
    "model.eval()\n",
    "\n",
    "def encode_prompts(prompts, max_length=32):\n",
    "    text_tokens, pad_masks = [], []\n",
    "    for p in prompts:\n",
    "        t, pad_mask = tokenizer(p, encode=True, max_length=max_length)\n",
    "        text_tokens.append(t)\n",
    "        pad_masks.append(pad_mask.to(dtype=torch.bool))\n",
    "    return torch.stack(text_tokens).to(device), torch.stack(pad_masks).to(device)\n",
    "\n",
    "# ---- Class prompts ----\n",
    "class_names = FashionMNIST(root='./data', train=False).classes\n",
    "class_prompts = [f\"a photo of a {cls}\" for cls in class_names]\n",
    "text, pad_mask = encode_prompts(class_prompts)\n",
    "seq_len = text.shape[1]\n",
    "causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).bool()\n",
    "text_mask = causal_mask.unsqueeze(0) & pad_mask.unsqueeze(1)\n",
    "text_features = model.encode_text(text, mask=text_mask)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# ---- Image preprocessing ----\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ---- Load one image ----\n",
    "test_dataset = FashionMNIST(root='./data', train=False)\n",
    "img, label = test_dataset[random.randint(0, len(test_dataset) - 1)]\n",
    "img_rgb = img.convert(\"RGB\")\n",
    "img_tensor = image_transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "# ---- Encode and compare ----\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(img_tensor)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    logits = image_features @ text_features.T\n",
    "    probs = logits.softmax(dim=-1).squeeze(0)  # [num_classes]\n",
    "\n",
    "    topk_probs, topk_indices = probs.topk(10)\n",
    "    topk_labels = [class_names[i] for i in topk_indices]\n",
    "    topk_texts = [f\"{label}: {prob.item() * 100:.2f}%\" for label, prob in zip(topk_labels, topk_probs)]\n",
    "\n",
    "# ---- Display image and predictions ----\n",
    "plt.figure(figsize=(5, 6))\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"An image of a {class_names[label]}\", fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top-5 predictions below the image\n",
    "for i, line in enumerate(topk_texts):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
