{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>\n",
    "\n",
    "- [Helper Functions](#toc1_1_)\n",
    "- [Vision Transformer Model](#toc1_2_)\n",
    "  - [Patch Embedding](#toc1_2_1_)\n",
    "  - [Positional Encoding](#toc1_2_2_)\n",
    "  - [Multi-Head Attention](#toc1_2_3_)\n",
    "  - [Feed Forward Network](#toc1_2_4_)\n",
    "  - [Layer Normalization](#toc1_2_5_)\n",
    "  - [Encoder Block](#toc1_2_6_)\n",
    "  - [Backbone(Stacked Encoder Blocks)](#toc1_2_7_)\n",
    "  - [Classification Head](#toc1_2_8_)\n",
    "  - [ViT](#toc1_2_9_)\n",
    "  - [Dummy Test](#toc1_2_10_)\n",
    "- [Download and Prepare Dataset](#toc1_3_)\n",
    "- [Training](#toc1_4_)\n",
    "  - [Weight Initialization](#toc1_4_1_)\n",
    "  - [Learning Rate Scheduler](#toc1_4_2_)\n",
    "  - [Define Training Loop](#toc1_4_3_)\n",
    "  - [Start Training](#toc1_4_4_)\n",
    "- [Appendix](#toc1_5_)\n",
    "  - [Position Interplotation](#toc1_5_1_)\n",
    "  - [Visualizing the Image Patch](#toc1_5_2_)\n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Helper Functions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def expand_tensor(tensor, dim, head=True):\n",
    "    while tensor.ndim < dim:\n",
    "        if head:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        else:\n",
    "            tensor = tensor.unsqueeze(-1)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def visualize_images_with_labels(\n",
    "    images, labels, idx_to_class, preds=None, num_images=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a batch of images with their labels.\n",
    "\n",
    "    Args:\n",
    "        images (Tensor): Batch of images, shape (N, C, H, W)\n",
    "        labels (Tensor): Corresponding labels, shape (N,)\n",
    "        idx_to_class (dict): Mapping from label index to class name\n",
    "        num_images (int): Number of images to show (default: 8, max: 8 recommended)\n",
    "    \"\"\"\n",
    "    num_images = min(num_images, len(images), 8)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(num_images):\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()  # (C, H, W) â†’ (H, W, C)\n",
    "\n",
    "        actual_label = idx_to_class[labels[i].item()]\n",
    "        if preds is not None:\n",
    "            pred_label = idx_to_class[preds[i].item()]\n",
    "            title = f\"{actual_label} ({pred_label})\"\n",
    "        else:\n",
    "            title = f\"{actual_label}\"\n",
    "\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Vision Transformer Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    num_channels: int = 3\n",
    "    num_classes: int = 100\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 8\n",
    "\n",
    "    hidden_dim: int = 768\n",
    "    mlp_dim: int = 3072\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout_rate: float = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Patch Embedding](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=config.hidden_dim,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size,\n",
    "            padding=\"valid\" if config.patch_size == 16 else \"same\",\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        imgs: (batch_size, num_channels, height, width)\n",
    "        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n",
    "        \"\"\"\n",
    "        # (B, C, H, W) -> (B, hidden_dim, H', W')\n",
    "        x = self.conv(imgs)\n",
    "\n",
    "        # (B, hidden_dim, H', W') -> (B, hidden_dim, H' * W')\n",
    "        x = x.flatten(2)\n",
    "\n",
    "        # (B, hidden_dim, H' * W') -> (B, H' * W', hidden_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Positional Encoding](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1,\n",
    "                (config.image_size // config.patch_size) ** 2 + 1,\n",
    "                config.hidden_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input tensor\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        return x + pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Multi-Head Attention](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAxPv7zmlNBz"
   },
   "outputs": [],
   "source": [
    "def scale_dot_product(query, key, value):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    Args:\n",
    "        query: Tensor of shape (batch_size, num_heads, seq_length, d_k)\n",
    "        key: Tensor of shape (batch_size, num_heads, seq_length, d_k)\n",
    "        value: Tensor of shape (batch_size, num_heads, seq_length, d_v)\n",
    "    Returns:\n",
    "        output: Tensor of shape (batch_size, num_heads, seq_length, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.head_dim = config.hidden_dim // config.num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.key_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.value_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Project inputs to query, key, value\n",
    "        query = (\n",
    "            self.query_proj(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        key = (\n",
    "            self.key_proj(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        value = (\n",
    "            self.value_proj(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output = scale_dot_product(query, key, value)\n",
    "\n",
    "        # Concatenate heads and project back to hidden dimension\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.hidden_dim)\n",
    "        )\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[Feed Forward Network](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n",
    "        self.fc2 = nn.Linear(config.mlp_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_5_'></a>[Layer Normalization](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.weight * (x - mean) / (var + self.eps).sqrt() + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_6_'></a>[Encoder Block](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.norm1 = LayerNorm(config.hidden_dim)\n",
    "        self.norm2 = LayerNorm(config.hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Multi-head attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = residual + self.mha(x)\n",
    "\n",
    "        # Feed-forward network\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.ffn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_7_'></a>[Backbone(Stacked Encoder Blocks)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_8_'></a>[Classification Head](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n",
    "        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Use the CLS token for classification\n",
    "        cls_token = x[:, 0, :]\n",
    "        x = F.relu(self.fc1(cls_token))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_9_'></a>[ViT](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(config)\n",
    "        self.positional_encoding = PositionalEncoding(config)\n",
    "        self.encoder = Backbone(config)\n",
    "        self.mlp_head = MLPHead(config)\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        imgs: (batch_size, num_channels, height, width)\n",
    "        Returns: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.patch_embedding(imgs)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_10_'></a>[Dummy Test](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "imgs = torch.randn(8, 3, 224, 224)  # Example input\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "config = ModelConfig()\n",
    "model = ViT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "output = model(imgs)\n",
    "\n",
    "assert output.shape == (8, config.num_classes), \"Output shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Download and Prepare Dataset](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWbWaT_llNB0"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get('KaggleKey')\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KaggleUserName')\n",
    "\n",
    "!kaggle datasets download puneet6060/intel-image-classification\n",
    "!unzip /content/intel-image-classification.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxBRrsVOMNsi"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=\"/content/seg_train/seg_train\", transform=transform\n",
    ")\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root=\"/content/seg_test/seg_test\", transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C5ZqXFVMhyk"
   },
   "outputs": [],
   "source": [
    "label_map = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "visualize_images_with_labels(images, labels, idx_to_class, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Training](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Weight Initialization](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Learning Rate Scheduler](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # cosine decay\n",
    "        progress = (current_step - warmup_steps) / float(\n",
    "            max(1, total_steps - warmup_steps)\n",
    "        )\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[Define Training Loop](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcinW-LgN-fQ"
   },
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        acc = correct / total\n",
    "        loop.set_postfix(loss=avg_loss, acc=acc)\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "            avg_loss = total_loss / total\n",
    "            acc = correct / total\n",
    "            loop.set_postfix(loss=avg_loss, acc=acc)\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_4_'></a>[Start Training](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMZ89wEhOiR-"
   },
   "outputs": [],
   "source": [
    "config = ModelConfig()\n",
    "config.num_classes = len(train_dataset.class_to_idx)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0WcsoYROw8N"
   },
   "outputs": [],
   "source": [
    "vit = ViT(config)\n",
    "vit.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(vit.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, warmup_steps=50, total_steps=2000\n",
    ")\n",
    "\n",
    "vit = vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2626aaadd80a4d4380e13bee8f122a41",
      "fc646a8f30ca463a8288dfd728436ae6",
      "f35a52019e894a5b84ee26798efcbbed",
      "ba14c03e40f44eab8e54a87a7e2625c3",
      "aecb050fd87a42f39377388e0e43a66b",
      "043b5171b08a43778535536d4bbc46ec",
      "103f9ccb9bb64fe5963c193ec56bdaa0",
      "24d28f3d9f7249838464f65e92c1336c",
      "8a5374cdd39c4b1d8c74e13bfd186772",
      "a62884e518f240dc860344e71dd3d751",
      "7ba331a8d39d4c6ba8e3a783749421c1",
      "622c636352bf4a8496ef7d50560b4a25",
      "94ee043a0c2e4f708720cb07e357e810",
      "ef14a943c9ce4ca38e39134999eb99c4",
      "cbcfc38ed31d4abc8a86380a0ca46c6d",
      "922fa9ee43a948f5875289b86d8db3d4",
      "f62d7d9b430f42a4814c96d980d9bd0c",
      "f8ac9a5fcfd64fcfa44cb5a9823008aa",
      "9c6fdef979a344a998dbce8b8b88f0b6",
      "668fc7b0f3634985876fcd80ac7920a6",
      "64858035bdc94f2c8bcb6c08a8f69078",
      "ef5e0ca877ba4b2d87193fbfc7d120a3",
      "cc7cdbea883c4d80bc0c248762108191",
      "673bccd60b0b4f608b0d1dca66c039b4",
      "b469963322bd4ad38f6c980f253f7984",
      "0c37a4bfcc814e1ab528fb9c9d50ab4a",
      "b2c12fe9e40f431e91e5bee4fd86ae2b",
      "99b8fe11e6104208963c1b779685855d",
      "3dbf42817954442d9cb16e6ef0654feb",
      "5ec12583c99e4049b9173e22dc24217d",
      "441aa5ef837c4bce9bda9219780c7ebf",
      "ab89b8f9b41b44049bb7d1b8dea462f1",
      "77b56115110a4c818bdbe474687e6d23",
      "31b616718dab4ea5833b5006f4f47b76",
      "b9825fd11aaf4da7b3d9e65898fe89f0",
      "8264a30a1c45420ab947d0a53288a51b",
      "b92514d9fb764eea87ab31cbb6eef36f",
      "c313011849c2427fb9416b712139e664",
      "ad3b3afafbda4c1881b07a00433713e8",
      "871fcd806b594415b442dfebe18426dd",
      "c277194638be49c8b8a69c05ecd4e234",
      "b89a16bf00bf4a4f995f836a5f1eedd7",
      "196e200e94c54851bf011ccb830d8cac",
      "4ec8ae22d10b4c0b9ba29f76789f34d8",
      "6adccd7e60354cdf8f35ac10cf4ee2e2",
      "275cd571dd52429c86041986831336c8",
      "7abd1c7bf29b45b0bd0fd3ffaa25c058",
      "ce7324a9b22c4d0b87eed25182da3411",
      "32a9d7afe2e24437aed131cc84405e4f",
      "0f1f60f0f200415ebcbf0ec3b891e399",
      "ae1e15cfecef43298b977029cf1ea17a",
      "5d66cc755ad24059b54d53288517e40e",
      "3cf40974c6094225b655fe0f150742ac",
      "9ad4285f3a81426980128f7a97817540",
      "607d782b19ce4872b1ca5cf6bd44ec06",
      "ed817a7d9cd4474d86a89642d3bd0601",
      "0106e97bea3d43e883bb43e40ce0cde5",
      "1b817191898f422abe19b7687ad0fe28",
      "30313d1585454d6ba22f3a74af271af7",
      "fc6fe435865e4a5faf42c29ab2308f12",
      "8d4fd30fb5924e9c8cd5a7d7097204b7",
      "18fdca46caf84fb7b476f7a75f244e97",
      "f2d6b219baa04b69a22f7b0cd8a758c5",
      "6c090fc495a2435b88f75d780bd57492",
      "5540f280958549c989d03c39c97dbc90",
      "114ffc381cbe4c9fa1244339531960ff",
      "2d27209ba39a41ba876be70eb2e088ef",
      "240101a2942942b885fa49d23e8b8167",
      "f66c16796aa541eab44ed5de4debed0a",
      "dfc5089ce26f4ecb8b509b1228d9fef7",
      "813b21ceee8541319877a009bbf12c0a",
      "5ee8f45b511a459d97313dcc9d330afb",
      "61b2a4ba060c4ebebe92a76cb49adff6",
      "7ab92cd54d37451ca66ae516b71d905a",
      "cbee5b637d814c96814a4d5f137d1c44",
      "f2781ee972cc40ca9ce4a6e3e5eb261e",
      "f1b2dd6e5d2f4eac8b098a38368b6295",
      "e7eb9a7161b64e9ca8a799716377ca86",
      "0e368ae9dd2a461080f86cbba7fefd12",
      "2f2342f1aa4d44a5badf6cbd68bcc4ad",
      "d2c11a9418294b74a32249c82522ee62",
      "ed3a09dc02554e80962f3baaece103b8",
      "d72b7619c3ad4a508c4c4b7dc0aafd92",
      "736bdce41d39424798c9c0d029c3f0fd",
      "8e9be13f55944b68bd7d13378c5dca83",
      "62e41a91a39a4812a9e2eca8a8bf4288",
      "22c0b24b8d9d4bb599d42f97e13c7034",
      "6f5d6e095c8044c4bbf0e7326ea56ab7",
      "9b9bd503d61e4982977d371110d2384e",
      "a481dfcde7e14960a03ee2a9e03a09b6",
      "ba95533f2a3a48c59c090acee0613c98",
      "98a3063b9b974730a7128e09b9da1cb8",
      "6ce81349e5e7413194fe1becec741e93",
      "d1ca78792cb14164a3f7a4ea840c1c74",
      "539d96a1da3b4be9874e1fbc0dcfe5b4",
      "09005b19168d4bef985e1285255a5af8",
      "4313e3dab4f24e5ba6d0dac70312eea2",
      "834c506ff4294fad9ec83c980bd2d5ee",
      "3f4cede21b4c4efca6fdaf9f4ed94114",
      "9ddb3c373a8e4134a72d0deb9b8ac187",
      "8766f852d3504f0086d947cecacd8a3b",
      "44b98b41af9941f0bdbff715bf6c4d4f",
      "29756755570f46b28df0627879b7c82a",
      "b8aa3711b53e4463b72eafaa396a1213",
      "60f76412fa0c4663af1df92ba4dbded6",
      "22ffd6a21bde466c9db06d0f563d2c05",
      "d79ae4c0dac74635b6887ec6ba4ad997",
      "22d070465eb64b88b93fd730caa54075",
      "95bf249fc2114ebba95077979e4f8e01",
      "fe7d9d721229419cbaf16339e57d70a2",
      "2903d22e970f4dbf803a97182bb0625f",
      "2230aa54cf6c4728a9888e633500c117",
      "494ed850fad04f0b8ce543b1e8a23297",
      "22e877d8465f4d448a80e53c4f6d3c73",
      "83d07a576f3046a4a66cea249d6a8535",
      "7acfc181bc554631af95a62d12fa7c77",
      "f65d0991fc9a448d9d4cbc780e3b13e1",
      "6cd8ede4b3744a5aa9c5eecf020cabde",
      "b8b3d0e55c7e45b7b0a478beaf581514",
      "0d4971d442a1493f84297deef775d773",
      "c3bb945095be488db27b09a2e784feee",
      "04f550774bdd4410a8246a2f6308ad8e",
      "43dae68dde6e4b25898ed94a2d3546c5",
      "9ebccae2487348509468c87c7d9bfe34",
      "52b1631b6e64483bad9c7fb4b25868d0",
      "17650050821e469d9628170db6aef9e5",
      "067889d2b6774617ab3563dd4582f1ac",
      "f79442bb792a40b686fcd0f1e03ab1e1",
      "5fd27c29b75045d5863129c9f79afdaf",
      "8646299451ca46f581f32d6e82323150",
      "49b22b1a68a44eea92a9af2cae41c9da",
      "523815a10c9e455290d9057a1c7e1c58",
      "b00cd69f867b4d91adaa475ed77e240a",
      "6908f9c14ca143099fec71434e3b52e4",
      "a30f5c8310664ed8ae0fc071d6b85aa4",
      "02d4f032202b444b9aecd8c7a421b6d3",
      "316dfc1514114fb7986cc8f443fc862a",
      "1ea418334acf4e32ab5aff004a6a064d",
      "3ee1dc998fac48c7a6d7aacd9eb4ab82",
      "cb8dd399149b4958a53cd7718965ee7f",
      "1406999e587242b5817028e6c3e272c5",
      "3532f8b6151549be802c34a33cbbbf81",
      "d3d2e9ca4e88410d8c06495c8be76d89",
      "65722542bc394d378c8c06330d131b32",
      "baf853cb75434987adac19c71f39c7f7",
      "ef4a1890a1f942838dc17072fec9ba19",
      "edaf5250d4ba4e6e85f920aa01419d47",
      "73a81c6780a34b4882eb1db232748a17",
      "f23890d458d0493fa6868f7c5e36ec07",
      "dafe37cff23642bf9a7c26a4e28fac19",
      "1a8533b6b7004d9e988a6f26d8040952",
      "ed0fb29cef334b17ae2d1a9d252ae2a7",
      "2741d6bb1d3248abb7d63acc84e6166a",
      "03f93bcb12594e2da196a65d87f78fee",
      "cd1cb3706506481cb6717045a164341f",
      "31f635b9273b49eda99d2cc785f305bc",
      "19843b7c33b248409e559807afdd1079",
      "eb4a03f14a7f4d858dcc9ebad81e8bd9",
      "de8f9c54590f42a98130109cb49d0f45",
      "93aceff54881473989a35f23c58dc47c",
      "cc58926ced214f7e9b0160cf028b87e3",
      "604219ca1a1a411bbd4705d3d2d89418",
      "54ffb022f62b451e92ca95e2989f376d",
      "8752a93e0ff94f72956eec4184a6ad89",
      "5b3fa08541be4ee5a5c6372a84dcfaf5",
      "959aa1bffb58490d9f32e8ea3bf4a51f",
      "8bb7b964ddd94ac5801f8a89d4e0f0e4",
      "5a9850f705394383896c3d7092fce3f6",
      "3943a11b41654b5487883dcfcaeca379",
      "8909922ff43545759f199d4c6b2aa0f3",
      "3f1bc21f50c74eee98cfc4478e5bfc35",
      "a3770a7d94cf4d76a67f4d25e379010e",
      "e2cf2fcdbdcc4edc8b3f495ecdd442f3",
      "fa9059bff4294f8ab564256402acc416",
      "589163009d0a40d68a15dd9312e9bf7b",
      "b6a22bea06ca490db89bff634286caa6",
      "398c24cc3a9e45d2987b4d94afb7b35d",
      "916fa40e45bb48f995929506149cff91",
      "555bc512946144fe970e01014680f2c1",
      "b9d3292d3c7e4e058a82ac52547c5786",
      "b8a8a5298a2c4e05942780d5924d4047",
      "b039a3d469ab43b4918b97ec3aee46d8",
      "89506d2ecefe4a1aa4c9eadb11d41e5a",
      "12a10c643ed04f66bae08e8c34367ec8",
      "2aba0920615d4df1a29c3c24a174a52e",
      "3f4a8329fc9449da883a104be0d40070",
      "130b7ab0e23d4744891bdd02816941f6",
      "28c3b64b1a744534aa3003f2b1cd52b3",
      "a3d42fad76e24e07abe256e55884ce11",
      "9ae1caafd1d14ed4aca5be1d984b963a",
      "e96da7caab8940ae8370f01108c8c029",
      "0d73c39b429a494e864135ae7c3352d0",
      "f8006d4011c847d2be422464753f6dc0",
      "55e1b339e8ea4a3d845cbaf36e454a3c",
      "fd51a07949504d698288719a4ce85949",
      "144b4d7f108a492391ab269a9c016489",
      "1477dfb7028842c99100e05105d06d23",
      "c9d10c65c1064713a5351087066d1b97",
      "5d46f79289a04c0e91960e38606a9d4f",
      "47510419fb264d28a24b3dc5025927c2",
      "5beed2c0982e476ab41b930920bc635e",
      "974c8429aca046f0bbc5a1b8d5c204d5",
      "39d0b60c9e9b44289e5d0326f7b0e86d",
      "f6ff773217dc453d9d755c6a1643ab3c",
      "52fdd49aee8846bc9928acc248981209",
      "b4d9b6d2294440a0be6c39f26eea092f",
      "d9bdee9f5c684dd4aea95cc75b864dbc",
      "118df8bfea5645798734d50a3e9ce342",
      "7a8cd986a1d14b21b8d73638dc0167fb",
      "b4f7f157a70042e3a4ddda222dbe7095",
      "00cab80829ae4ec2871c7f60ef1743f7",
      "28e51f585b8b4752bcca6f1897e6aafb",
      "ef24759fc9f9407ab91877e2e1b90638",
      "0174e72584aa4f47a7da0ef652bff5b2",
      "64f470c9323443fcab9d39b174ee24b2",
      "30d99a41dfea46d98e2ca675da8f388e",
      "dd0b9fc87d6342cf9017b3fa5ad05326",
      "2c848c070a5e4e5daad370a1dfdacd11",
      "373b0d8a712144f897a7c4e172ba7c10",
      "d2b422dc0d8c4448b17a32eec64364fb",
      "dfb66c4bd87642fab2dd7c718623cac6",
      "7e6daf75509b4bb8a0527d68d1d13c4c",
      "87a44c9a3af34177847c25cbf78dc23c",
      "f78b935da3c94558b3d1f4f9037ef6da",
      "48542f73c54144d6b05624677aa89583",
      "dfc3d03348d342ddb0beaf2d423a19c2",
      "3f0de752cb114f6f8a4f857ed90afee3",
      "649b8cfeab3b4396b88fce6d94d038bb",
      "25b270a000ab473ba6eaa14372f3707c",
      "b6b816bf36fb4b62883217794c1ce8f5",
      "2ee654671eac441f8647470cc7d4d305",
      "a439b6a9abda44ee91dc0e8e982be52c",
      "a99e74bd93c44cde8d48252616c9e68b",
      "ccec80c97fe54ba297c8dea492edb15c",
      "ae73380f0715485ab898a33587584ad8",
      "c63502a4f9d34149b214db17a2313511",
      "447365a9852b403cabf45aad2f18ae4f",
      "90eb57107f894c0988b13f728e2c681b",
      "1796bc64efa84bb392f26593b952ac86",
      "30a22670ea4c40f49ef628ddee2ec6d5",
      "6273980094e447c99e8d264b90885d88",
      "42f8140132e14c8a80553ca194fe0f9b",
      "9166fca6ace34e87b838bd552dad2136",
      "d8371a7ec9cb49f09c5d373e3b408259",
      "6d70290e5e2246f881da814b23cd7a17",
      "95e48ef3a4b8426cbec6e992b23518c5",
      "2a4025b8cdf14ed38d9ea80ad47a48cc",
      "e313e60b8ace4e6f8ce73f17470b1990",
      "4b21b5bdbb864f87b1c483cd1979c9be",
      "1e440aedcb6f48a4be3ce32e9421366b",
      "d380587f480e4a029cf3fd3976aa506d",
      "7c8bc0085c2b4e96854b504b3eef768b",
      "1bdc8ab85eae4e58867eeca2efe710a3",
      "77dae3dfe02844dfa9d9cd3ba5dc2953",
      "e6523f279a804e49906a8c8ac631f7d0",
      "001b96936fdf4cd881f47e130e896272",
      "49084956007d4a469d8d4638485aa23c",
      "f209449d67664710880a5e45ae917e8d",
      "058e9a59a17646ecb10ce35b61d0febe",
      "eed6fc4c3d96416b8166a35c96f59066",
      "9d9c333a0a454989bd324370421fb7ab",
      "b11a1e446eed4372b34e8f41a2c82ad5",
      "4550218ffe94400ca5f6beca7d4198e1",
      "f40f8b29c9d74d798b3c7af1d92b5c83",
      "30952455beec49a8995e1ec41288fe8b",
      "8a9ae17ea4ef4367b3b4511a62e9d59b",
      "721917d31fbd48259d83885baa22861e",
      "7835340768054a3392510259c82e90e2",
      "2c2858aa93a54e29ade2228b46d9663c",
      "78e99d3e63e94aad9b9db0a764c3c35a",
      "5f5f62139ee24ea19143b4643c2d3422",
      "f4675b4e2ab64101b623b68ba4f0c509",
      "77ce144f0cb44372bc60c5b43e617cac",
      "c75f641991154aa29817e0d996ef3286",
      "5448dfba0b3549a8b445b2934bab749b",
      "7ad4e875362b4b2aaa3c8c03ecd0d010",
      "12e8c2cc3d91429c80ece98382a70815",
      "201145e9471c401bb79fa8a5192cd9dc",
      "36a597ae6382463483815e98628052cc",
      "227d01d6479a4862b07ac2f1bf229fb7",
      "9c51b2c7e8d8444887f2263b27f1e761",
      "46dac08372d04b8791b8955b9a754982",
      "e669610340f4447db783542c3c9bdbfb",
      "ccd67a082f674301b8b78a69157d6875",
      "b4bd4323ef9b4833aec265b19d1249e7",
      "70d5706acd1e40fdb9e5ad663465001b",
      "8e6755afe49a4aa5bb3509994c7c0346",
      "399a83bd27884c72a5426df5356c6969",
      "291b8196546e42309bdd3a849f2fb9b8",
      "3479ffec09954f3b92dbd438b24bf0f4",
      "bb81feb873ce491484c4680e071026a6",
      "33dcc3242aa44871aa6035b34cc897f5",
      "ee47c4b5766d46d7af86d092c00fa0eb",
      "fca6efef495b4cb0973a89ed2d9633aa",
      "6ab2f1d0a76844faacc926cc1e31ba88",
      "ac0c9568b45d47b7a49b77bd73f6d1f4",
      "2c230d7c3ac04c8eafa1f96e3bf3ebcc",
      "9ddcd0c0d9d64a84a6d8cd1527e81d78",
      "5860b51d73f54affad8be76c03af97e3",
      "7b4192c621294fc282332ce5ff53b124",
      "ada04aec4a0f41f5891459286dda2447",
      "7fa4128d486c46bc8da1283a274773de",
      "316d4145e5e44fa09e0012cb949d4d71",
      "e01b4578d6164251a064859ff7fc762e",
      "8528b99a702844469939583df9910a87",
      "ee2a55d9793d4a1a8a68d671d56f5455",
      "882c4d239a8e4a9bbc3324390ad9268b",
      "ff540d4dc69e4ca2b69314457567f8c6",
      "17ff73a552624b9fa4c9d764e03ec223",
      "5bec35cf51154744b07754a3150d0e91",
      "74322e2cd9b5492380d85b241e57bebd",
      "11c3006745174bd395aaf193cadce51e",
      "97640a2875c14d6c8f4dd8c90a04aeb3",
      "a4ac854a54de4a66ba44c618a31963c3",
      "65f870af935349f6afea9fa36b36055e",
      "de51a1a2347b4936aa822a26233cf0b9",
      "5df0aefe6297456684e3d1a75552df66",
      "d56e5012acac4aa8bc6341bc500401aa",
      "62125775cdbb480e8c4b9b92c075d564",
      "b2efca205f67477d9384e2aca5353dad",
      "5aeaf3c6754543aca3ed1fd0e0d374eb",
      "ec6d420d48f74c5186da7a9ffac25955",
      "f7cc3696159d489e833ce129dfc024c0",
      "c9ccf3e3ab114759bae4ae89aac416b1",
      "0e2ea86cd41046398ed534dbdf5b7f28",
      "4ec0b336cb7d44a0b5589356b4709585",
      "c4502e619dd24ee09828bf6c89567c1c",
      "7374aed02034453787cd05a7ba069e28",
      "41e5515b6d184bab96d7fb146bcf4df7",
      "6fcca693dcf8488eada91a5703191374",
      "cf5f90c925314b93a2809ecaaaf016d0",
      "259f1c46ff634d92b5327d64cec5494f",
      "11f68515505e4051926441bb60ef9b5f",
      "8fd1a97861124ddfb2e99c98451db318",
      "3ac66e878eee4fb1b4046de521e5f2a2",
      "37057bd35f444df48968a5ddeb6e4ac9",
      "5e46fd540fc84e3b8e3b8fdc3170b3ec",
      "185b55f6c3084088bcaac085fb8146fb",
      "dbc63a8cb56841aeb76dc26e8f846a3a",
      "c4cb6875699d47a485ed968ff63dffb5",
      "4f903cf84269436ca0ee9ec4b3115f6f",
      "b52733240f524887b247d8702f7e5593",
      "2d56355a6ea1473e9d6150d34746d79d",
      "e6e97f59806f41128d24bdf0bae86a02",
      "63e33d74054d48e6b41ab493f01ed23e",
      "c0933dc3c1e64a15866797dcefdd8598",
      "b15cb6b4f79f425e94dd7360b3c700a4",
      "ee1224dce66846d99e99b03eeb73fdf2",
      "19338b9b36a642229839a64423507d6d",
      "da46a1e22afd444e906313fcd78a1a18",
      "ac020e1e2ccb46ca9820aa08b20b6c90",
      "ce643cf3f64a45b9b1c32e8e017fc9c2",
      "b0a1883c50b34bf68d37a61998c3d545",
      "14a824da41984f5a8acb9cf3daea19fd",
      "f6850df513914c1e8aa17c961d264bd2",
      "f9cce1fa9d8e48608aca00a7bfe5847d",
      "90e7e140a65f42ebb53b4035e8406efb",
      "e4b1612bf7bf4e2f81dab210e44602b3",
      "7c77d62243a940319d1811a1cd626e5e",
      "3e885bb8038145dfad1f90bfa472ee09",
      "1d50c4b0251f469f919005f62cb175fe",
      "651024ccce1042bbaf70087291eb3752",
      "64f33d0d2a844740a24372d57ef53db6",
      "76d876d3dd5c4f3a86a70059e4d93ce3",
      "a33d1038e32a44da8e635d231102b19e",
      "241dcd21e57f4544b9a311c7d025319a",
      "6c61dab0a51d47d0970301459b593f1b",
      "219db5227e7a46d18cc42a823d601fdc",
      "c1de775465e049288d7abe007ef03321",
      "358ac910f6e441ba8541fef2604995e6",
      "612c1bfbc8424a04b2c245c113adf4e8",
      "3e766c23a8364e5ca0166227c735a1be",
      "8b91d533effd4c48ae8b36892e2801b4",
      "02ea8d19558845e2a0c975e5e4c615fb",
      "7768b53df4dc480d9b46f482ed5c83fd",
      "b8f2ae7c75114b0db7d77168c613902a",
      "326d053f97d74289b0cddfb14299e188",
      "841fd02ee9f340edbc8444e2aada8783",
      "cf0b84328eb146ab88418b55ea2af66d",
      "2d9e39f225514434852e41ae8a3ad0aa",
      "2a9979fe0a0c464480014a3c56e3caf0",
      "b924e64ecb8d4798b9d4cd0655c8619a",
      "056056427fb148958e0da77d9bf49a98",
      "317605d630604b0a9a512ea9809c4e4a",
      "a8651579db0a42d7bdf084ab5c1a4c1c",
      "d8fefe6f992348c28c26dd5d6551338f",
      "80cfc00d21e94ad792a71ead060b056d",
      "b45986f2925046c5be19dd4268440a85",
      "ca988d5e361a4b988c214ee2ce93318e",
      "d65f788d5bac4253a7581acf67d62bef",
      "e0527f3b24da4fe2ab82196220db95a5",
      "f9283092b32b4976a4e83b389bbf9693",
      "e9eb670ba6bc4a539a0a2e793ac846e6",
      "e8a17d04a1fb4192a9ca357341312d08",
      "259ac075729b487e947e514b272c570a",
      "6857e8bdf7fe479daa617d689a5aea42",
      "773ced5654d6416991739d6ba3199a90",
      "b3bae0a125624ba191b7d3d67b247b6e",
      "919e61d9d0734d8d8f679542563b4b90",
      "0c2604f550654994ba8554c7eb163657",
      "f5fa4fe6e01546e4992a53eee3d28bef",
      "c69768b990444cb39f30a71b14ea80b4",
      "e8bc63afff284e66b294fd4338b15947",
      "7fc1845c42414bcbb2b602be7f847efb",
      "8db32102ed864228b831e727f429b729",
      "b178a7c5f9b94696a7308ce4de41d230",
      "83ca755e0a254bb785a72dd6c8b07d4f",
      "d28b41e708eb45ac87642943e8523114",
      "9a932946ae1d465ca9c6d608800008ff",
      "74e231f7ef294d0eae04577ff42b2272",
      "7925ac1175c9466c84fa81ba6a76b5ec",
      "7e8dddd787f14a06a9ace401a97234e9",
      "d2d7483ade4943778b645d74ee1fb48f",
      "db5c89d925b3443bbacb8e81c156e610",
      "b18150d100824effb68cf85315d3132b",
      "61084cb0159e4932b2be33a42579b627",
      "2a7dc42d9f484b57a66ef7e068220522",
      "157927b5b8dd46aa92cf98f17bfb5859",
      "096c90650a03490b8b9706b3ff8a9589",
      "3d21ff6ec5194b0ab2b082eb25bf0910",
      "cc6fc95fef45488e8387deabfa17ca36",
      "afb37f1c329746f5a1ac43744d21f968",
      "93c4f88b32264f6bb5d0a7baf7dde0b9",
      "a253342c8c1748c7aedb60a48853be9f",
      "916586f5b96d44fbb8b9ecf57e4eb859",
      "1d3d5796830947ba834a8fd01928d051",
      "b7444c45cc07423a841d06b441c22e47",
      "96d205b48628489781603633382554e3",
      "7d06f63567d34f7cbc4c9218eeb8a826",
      "e0a0ab58b4864149a9bb7fd61214e192",
      "ed60860f772f4c10a2b32a42928e5873",
      "e368948bbad3457bac8a6d99ea59a20c",
      "b163873a4b704d2db329b50926655c6a",
      "6c4f72450f654c41b8a5905fb116ac23",
      "43252692daba411092f761b33628c40f",
      "7aa34e71be9848e2a0b0a9bcd809eeaa",
      "0f17d972343a426cbaf23d108d687f05",
      "56d28504a4d34c559713c85d406af88e",
      "a61ad2b4bc124ce99e7b35c1f289b046",
      "1fe5817d49c4480d8d33f14be4ad4fb2"
     ]
    },
    "id": "poWmrk-gOOTa",
    "outputId": "9f593d4b-f738-4e5c-88cc-bdbe009f4155"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_step(\n",
    "        vit, train_loader, criterion, optimizer, scheduler, device\n",
    "    )\n",
    "    val_loss, val_acc = eval_step(vit, val_loader, criterion, device)\n",
    "\n",
    "    # Logging\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGRPhaf7lNB-"
   },
   "outputs": [],
   "source": [
    "def plot_training_curves(\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, num_epochs\n",
    "):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Acc\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "Ps8kjFt3lNB-",
    "outputId": "8942174f-e417-41bb-8a6e-0122853642c8"
   },
   "outputs": [],
   "source": [
    "plot_training_curves(\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, len(train_losses)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "preds = None\n",
    "vit.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    outputs = vit(images)\n",
    "    preds = outputs.argmax(dim=1)\n",
    "\n",
    "    # Move data to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = preds.cpu()\n",
    "\n",
    "\n",
    "visualize_images_with_labels(images, labels, idx_to_class, preds, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Appendix](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[Position Interplotation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZXPXvSfKPU-"
   },
   "outputs": [],
   "source": [
    "config = ModelConfig()\n",
    "vit = ViT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Kbc7DftKOTF"
   },
   "outputs": [],
   "source": [
    "orginal_image = torch.randn(8, 3, config.image_size, config.image_size)\n",
    "original_out = vit(orginal_image)\n",
    "\n",
    "assert original_out.shape == (8, config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRWEYTLAKel-"
   },
   "outputs": [],
   "source": [
    "def interpolate_pos_embed(pretrained_embed, new_grid_size, num_extra_tokens=1):\n",
    "    cls_token = pretrained_embed[:, :num_extra_tokens]\n",
    "    patch_pos_embed = pretrained_embed[:, num_extra_tokens:]\n",
    "\n",
    "    old_size = int(patch_pos_embed.shape[1] ** 0.5)\n",
    "    patch_pos_embed = patch_pos_embed.reshape(1, old_size, old_size, -1).permute(\n",
    "        0, 3, 1, 2\n",
    "    )\n",
    "\n",
    "    # use bicubic to interpolate\n",
    "    interpolated = F.interpolate(\n",
    "        patch_pos_embed, size=new_grid_size, mode=\"bicubic\", align_corners=False\n",
    "    )\n",
    "\n",
    "    interpolated = interpolated.permute(0, 2, 3, 1).reshape(\n",
    "        1, -1, pretrained_embed.shape[-1]\n",
    "    )\n",
    "\n",
    "    new_pos_embed = torch.cat((cls_token, interpolated), dim=1)\n",
    "    return new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utlCgesYLJag"
   },
   "outputs": [],
   "source": [
    "pretrain_pe = vit.positional_encoding.positional_embedding\n",
    "fine_tune_pe = interpolate_pos_embed(pretrain_pe, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtTU4dqWMDm_"
   },
   "outputs": [],
   "source": [
    "# Replace the positional encoding weight, and also enable fine tunine\n",
    "vit.positional_encoding.positional_embedding = nn.Parameter(\n",
    "    fine_tune_pe, requires_grad=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbcaiP9CMQfS"
   },
   "outputs": [],
   "source": [
    "fine_tune_image = torch.randn(8, 3, 384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the number of classification is same,\n",
    "# else, also need to replace the classifcation head(MLP layer)\n",
    "fine_tune_out = vit(fine_tune_image)\n",
    "assert fine_tune_out == (8, config.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Visualizing the Image Patch](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHmdwPgVMgpH"
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import einops\n",
    "\n",
    "# # Step 1: Load and resize image to 256x256\n",
    "# image_path = \"github.png\"\n",
    "# img_bgr = cv2.imread(image_path)\n",
    "# img_resized = cv2.resize(img_bgr, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "# img = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)  # Convert to RGB for display\n",
    "\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f_MDlpLlNB_"
   },
   "outputs": [],
   "source": [
    "# # Step 2: Patch the image using einops\n",
    "# patch_size = 16\n",
    "# patches = einops.rearrange(\n",
    "#     img, \"(h ph) (w pw) c -> (h w) ph pw c\", ph=patch_size, pw=patch_size\n",
    "# )\n",
    "\n",
    "# # Step 3: Display patches in their original grid layout\n",
    "# h, w = 256 // patch_size, 256 // patch_size\n",
    "# fig, axes = plt.subplots(h, w, figsize=(12, 12))\n",
    "# for i in range(h):\n",
    "#     for j in range(w):\n",
    "#         patch_idx = i * w + j\n",
    "#         axes[i, j].imshow(patches[patch_idx])\n",
    "#         axes[i, j].axis(\"off\")\n",
    "\n",
    "# plt.suptitle(\"Patches shown in original spatial distribution (16x16)\", fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6Nn70Bm4m0p6",
    "xrTe3HojKMJz",
    "ijMOg3jhlNB_"
   ],
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
