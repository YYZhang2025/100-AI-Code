{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>\n",
    "\n",
    "- [Helper Functions](#toc1_1_)\n",
    "- [Vision Transformer Model](#toc1_2_)\n",
    "  - [Patch Embedding](#toc1_2_1_)\n",
    "  - [Positional Encoding](#toc1_2_2_)\n",
    "  - [Multi-Head Attention](#toc1_2_3_)\n",
    "  - [Feed Forward Network](#toc1_2_4_)\n",
    "  - [Layer Normalization](#toc1_2_5_)\n",
    "  - [Encoder Block](#toc1_2_6_)\n",
    "  - [Backbone(Stacked Encoder Blocks)](#toc1_2_7_)\n",
    "  - [Classification Head](#toc1_2_8_)\n",
    "  - [Full ViT Model](#toc1_2_9_)\n",
    "  - [Dummy Test](#toc1_2_10_)\n",
    "- [Download and Prepare Dataset](#toc1_3_)\n",
    "- [Training](#toc1_4_)\n",
    "  - [Weight Initialization](#toc1_4_1_)\n",
    "  - [Learning Rate Scheduler](#toc1_4_2_)\n",
    "  - [Define Training Loop](#toc1_4_3_)\n",
    "  - [Start Training](#toc1_4_4_)\n",
    "- [Appendix](#toc1_5_)\n",
    "  - [Position Interplotation](#toc1_5_1_)\n",
    "  - [Visualizing the Image Patch](#toc1_5_2_)\n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Helper Functions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def expand_tensor(tensor, dim, head=True):\n",
    "    while tensor.ndim < dim:\n",
    "        if head:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        else:\n",
    "            tensor = tensor.unsqueeze(-1)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def visualize_images_with_labels(\n",
    "    images, labels, idx_to_class, preds=None, num_images=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a batch of images with their labels.\n",
    "\n",
    "    Args:\n",
    "        images (Tensor): Batch of images, shape (N, C, H, W)\n",
    "        labels (Tensor): Corresponding labels, shape (N,)\n",
    "        idx_to_class (dict): Mapping from label index to class name\n",
    "        num_images (int): Number of images to show (default: 8, max: 8 recommended)\n",
    "    \"\"\"\n",
    "    num_images = min(num_images, len(images), 8)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(num_images):\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()  # (C, H, W) â†’ (H, W, C)\n",
    "\n",
    "        actual_label = idx_to_class[labels[i].item()]\n",
    "        if preds is not None:\n",
    "            pred_label = idx_to_class[preds[i].item()]\n",
    "            title = f\"{actual_label} ({pred_label})\"\n",
    "        else:\n",
    "            title = f\"{actual_label}\"\n",
    "\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Vision Transformer Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    num_channels: int = 3\n",
    "    num_classes: int = 100\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 8\n",
    "\n",
    "    hidden_dim: int = 768\n",
    "    mlp_dim: int = 3072\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout_rate: float = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Patch Embedding](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=config.hidden_dim,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size,\n",
    "            padding=\"valid\" if config.patch_size == 16 else \"same\",\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        imgs: (batch_size, num_channels, height, width)\n",
    "        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n",
    "        \"\"\"\n",
    "        # (B, C, H, W) -> (B, hidden_dim, H', W')\n",
    "        x = self.conv(imgs)\n",
    "\n",
    "        # (B, hidden_dim, H', W') -> (B, hidden_dim, H' * W')\n",
    "        x = x.flatten(2)\n",
    "\n",
    "        # (B, hidden_dim, H' * W') -> (B, H' * W', hidden_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Positional Encoding](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1,\n",
    "                (config.image_size // config.patch_size) ** 2 + 1,\n",
    "                config.hidden_dim,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input tensor\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        return x + pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Multi-Head Attention](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAxPv7zmlNBz"
   },
   "outputs": [],
   "source": [
    "def scale_dot_product(query, key, value):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    Args:\n",
    "        query: Tensor of shape (batch_size, num_heads, seq_length, d_k)\n",
    "        key: Tensor of shape (batch_size, num_heads, seq_length, d_k)\n",
    "        value: Tensor of shape (batch_size, num_heads, seq_length, d_v)\n",
    "    Returns:\n",
    "        output: Tensor of shape (batch_size, num_heads, seq_length, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.head_dim = config.hidden_dim // config.num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.key_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.value_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.out_proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Project inputs to query, key, value\n",
    "        query = (\n",
    "            self.query_proj(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        key = (\n",
    "            self.key_proj(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        value = (\n",
    "            self.value_proj(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output = scale_dot_product(query, key, value)\n",
    "\n",
    "        # Concatenate heads and project back to hidden dimension\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.hidden_dim)\n",
    "        )\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[Feed Forward Network](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n",
    "        self.fc2 = nn.Linear(config.mlp_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_5_'></a>[Layer Normalization](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.weight * (x - mean) / (var + self.eps).sqrt() + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_6_'></a>[Encoder Block](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(config)\n",
    "        self.ffn = FFN(config)\n",
    "        self.norm1 = LayerNorm(config.hidden_dim)\n",
    "        self.norm2 = LayerNorm(config.hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Multi-head attention\n",
    "        attn_output = self.mha(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_7_'></a>[Backbone(Stacked Encoder Blocks)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_patches, hidden_dim)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_8_'></a>[Classification Head](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n",
    "        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, num_patches, hidden_dim)\n",
    "        Returns: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Use the CLS token for classification\n",
    "        cls_token = x[:, 0, :]\n",
    "        x = F.relu(self.fc1(cls_token))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_9_'></a>[Full ViT Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(config)\n",
    "        self.positional_encoding = PositionalEncoding(config)\n",
    "        self.encoder = Backbone(config)\n",
    "        self.mlp_head = MLPHead(config)\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        imgs: (batch_size, num_channels, height, width)\n",
    "        Returns: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.patch_embedding(imgs)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_10_'></a>[Dummy Test](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "imgs = torch.randn(8, 3, 224, 224)  # Example input\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "config = ModelConfig()\n",
    "model = ViT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "output = model(imgs)\n",
    "\n",
    "assert output.shape == (8, config.num_classes), \"Output shape mismatch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Download and Prepare Dataset](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWbWaT_llNB0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# from IPython.display import clear_output\n",
    "# import os\n",
    "\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KaggleKey')\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KaggleUserName')\n",
    "\n",
    "# !kaggle datasets download puneet6060/intel-image-classification\n",
    "# !unzip /content/intel-image-classification.zip\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxBRrsVOMNsi"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=\"/content/seg_train/seg_train\", transform=transform\n",
    ")\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root=\"/content/seg_test/seg_test\", transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C5ZqXFVMhyk"
   },
   "outputs": [],
   "source": [
    "label_map = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "visualize_images_with_labels(images, labels, idx_to_class, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Training](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Weight Initialization](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Learning Rate Scheduler](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # cosine decay\n",
    "        progress = (current_step - warmup_steps) / float(\n",
    "            max(1, total_steps - warmup_steps)\n",
    "        )\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[Define Training Loop](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcinW-LgN-fQ"
   },
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        acc = correct / total\n",
    "        loop.set_postfix(loss=avg_loss, acc=acc)\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "            avg_loss = total_loss / total\n",
    "            acc = correct / total\n",
    "            loop.set_postfix(loss=avg_loss, acc=acc)\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_4_'></a>[Start Training](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMZ89wEhOiR-"
   },
   "outputs": [],
   "source": [
    "config = ModelConfig()\n",
    "config.num_classes = len(train_dataset.class_to_idx)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0WcsoYROw8N"
   },
   "outputs": [],
   "source": [
    "vit = ViT(config)\n",
    "vit.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(vit.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, warmup_steps=50, total_steps=1000\n",
    ")\n",
    "\n",
    "vit = vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335,
     "referenced_widgets": [
      "6e3d17dcae8e417f90f056f88235479e",
      "04dc2d19f6b844fa82a1eb8ec16ecc0e",
      "4086b8f0707341ffac28c6f2146555bb",
      "c5beeeee63974860b03980ed168d0ef1",
      "dc408832453b4cc8865c8f225cb8a6ae",
      "1023f96eeebf4269ace8d2c2b03f057b",
      "521fe5f6fbed460196e31ce7b822f8a4",
      "0c133733fadf4de29d9788ee7679d8a0",
      "ea5ab74101e747bbacd629a5a0fb3dc7",
      "fe592e2d12e74c11bf72e8ebe787594c",
      "fc238a45220a4bacab8c73130671b5d4",
      "0d9cb5944a1940b4bdc473d091f5acaf",
      "56ffbedacc7a4f0abd5ee1a762fdd1ab",
      "337d1111182547c3b1ee4cc419735f2b",
      "08bb4f3fbdac4743afac4efa98a100eb",
      "6489758befa442f8895d75934ea27259",
      "615d430931f44952a4e3b033a1fa30bb",
      "dd479a90d4144414ad946a826dcb55fc",
      "c6fa03fe83aa44bcb8ec16c6a928108e",
      "d6e8e97b7686476bad409171cb48e6d5",
      "ffee50723f544bd6b9238875afce6198",
      "e9b0074deb2d4e19b310f031648c4900",
      "fbdc00cd430648b7bd43f3882f401e63",
      "0f04ee77bc3e44d5b67f9d272db53dd1",
      "7d08a3f5fddd4d0d9a2aee12c4101d13",
      "6992838f4d4a4c55a5346650bd798ec7",
      "6e0367bacf1e45b5af502d39cca22ed1",
      "67a77e4bafb44b27b87050c1f7555131",
      "5f9fef7d3d77446fabd42a211ea4ea34",
      "9f4c83b7dd2f4ec09658c4bad5571bcb",
      "a0f8a66848d1407581a633bf89bf18bd",
      "aecab0e1e65c4f75b3f6d0cf5b50b2f9",
      "79a7a457ade84ca4b48368e89a328891",
      "803a5ca46c8d4a5f89e7d65375a2fafa",
      "44fe36818fa34f20a8d1bd5b85b23c2c",
      "de49c6d1b26347138a93387a723b7a40",
      "4da3336f83954b379044a97a586635f7",
      "43e12c7cf13a415f9d3974a27e6094e6",
      "01cc9bab38444b20b7a384031fe492d1",
      "4dc6db31483941eca4c8793e9d5767fe",
      "05769ae5e7b944f08190e8f2375af873",
      "565b5ab18bc0499da257096db8220753",
      "24e1fffaf0bd470d9faa5e03c513d20c",
      "c7094e28785a4b5fa3219802b25d0e0b",
      "babdeeddb57a41838ab85c348d37cc5e",
      "d535dc9660ba439cb62c14e3bcb0e6ff",
      "627cd875e1fe43d8bf640ce2223b457a",
      "45fd121335f84589882b4ff62b9a3528",
      "ed615cd97dc34ccda38bc9d0c879577d",
      "fbbb5ba79df943ecbac9e7a29b9f0357",
      "e3514e1231af4286b201c924a624c8ac",
      "0593b8ad41c2403b811fc8663101389c",
      "284275afb2c0430c828f0a197f4cf6cd",
      "bc8ae63a8ef8432bb5013ac63f7659b5",
      "ccc5ed50905c44e7ad56d0a31635f6d2",
      "0fb50ce19d2e46608bb15387d65eb0cb",
      "efdf6e5370d544418aad8c4e7648d933",
      "85ae8aaa3d3045669c9750e3ee19661c",
      "8bc89b15403b4176b5f973a0afce3205",
      "a844ca6ad3334600a909c8d452e31b94",
      "2e0d56e060af482ab079f66465d5e483",
      "ea71c00aa0c74640930ea48b1861e52e",
      "45df95eaf8bf4896b22dc54bdfd6e407",
      "37a6349acbf74978ba7db79fd78903af",
      "e7a8f89ed7e44aadbd538dbdfb205f4b",
      "8edbc6fde62243599c855119813211ae",
      "73a190bed62a4e0ca574ea84c7b31f06",
      "28ecad82266248ea9550282c2512b9c2",
      "c68658c89d11417b9fd5b698928a369f",
      "b58cc18b2b8b487083e444bd66bf7e48",
      "9a6b2b64e5e841cc856b270e0ac83127",
      "d7fb933ff6f64d01899357889353bee9",
      "60feb6580c6f423cb66e0e365fa039b2",
      "6b694d6eae1d4722be88db450c2c24ff",
      "533fed06e5894c25bf329e26c17ed064",
      "158acb2efeee48939dc0404f499bf4a4",
      "aededd0d3a174d6f9f5baf9a81d98646",
      "9c17c2da4bd14fc68e61c1786eb97314",
      "94c76975c26a44eeb364e753369f4e2a",
      "7fed6615d25e43b28df4cc4d38c269c5",
      "cd7f9dab2205448fbf7e4ded43d23929",
      "356eb3dfe4494178bd30beaa8095c107",
      "cda94eb671b4413d9bef80447c237330",
      "1d7507f000b44f59a4cc170b8c245a5d",
      "c8469d2c09d54abfa64619037c27570f",
      "c785641ffdeb479086b54ebea685f327",
      "bb2077f8c79f4575ade201019cee9dad",
      "e625da7946b0483bab0f29af7ff5039e",
      "d748191d898f411886f1f8aed810886d",
      "340536253d294fc69899b94ad17fa144",
      "169ae9bebb1641b7beb6829666647339",
      "9f73b54df23a4502b4c243820bc63939",
      "aebcddeb0ff64927b78aa96fe6305c10",
      "67ed24c366fe478aaf69d6f6eba3b2a0",
      "1d54f2185eeb468188fb1b8e4de1f141",
      "a6d1954110ff4ceb96330232af28f40e",
      "04cee6f495784ea989f0f65fea368283",
      "ba30560274b64e858241454c7311c987",
      "f9a68c187d7d4336b4066fbefadaa6e5",
      "1e89b5c6a4e64b87b718cd67168bda1d",
      "508316c9f6ec4cfc9f4ec93c26b0eacb",
      "fdabb0dd2c10446c8a5839c0f84ae61e",
      "56793f79fd8b42cd863bd76630625b5f",
      "e26667311a6f4d049a164c5d765b57fd",
      "3b451c79d16c4cd39798991d499b7cb7",
      "148820eb377740239289f1dffe41374b",
      "5b6fad7058fb409d9745df3e4d6107d0",
      "270ba60296ce445c854ed124a8873c5f",
      "c11c0c8290e84e7aac8238ca2bea28de",
      "b28f2048c094439b8f9000baed239165",
      "6319aa26f56e4637a6c25ed28e8b8697",
      "367840a79f9b42af9e650170e51059fb",
      "dc55ed7489274b6abc153153ed682273",
      "1f2aae151c5e4814a4ffec0ccf5aa8c4",
      "11bed73fabed4b8aaa5ce0b0151d9978",
      "6ca88724e11643f69ccb06b715c2897a",
      "9e43f2e64c4145609f52f5b2a6392c54",
      "42d1e50f9a2f43b1925ead302eab9a00",
      "bd19457633064e2c869c6cbfdddb7987",
      "d933f26c05fb4d088dcf9c7db1da1e7f",
      "9c2c64df49b64b6cb8118cb420da7ed9"
     ]
    },
    "id": "poWmrk-gOOTa",
    "outputId": "72418f3d-6898-4f58-edbf-44418d1de96a"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_step(\n",
    "        vit, train_loader, criterion, optimizer, scheduler, device\n",
    "    )\n",
    "    val_loss, val_acc = eval_step(vit, val_loader, criterion, device)\n",
    "\n",
    "    # Logging\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGRPhaf7lNB-"
   },
   "outputs": [],
   "source": [
    "def plot_training_curves(\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, num_epochs\n",
    "):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Acc\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps8kjFt3lNB-"
   },
   "outputs": [],
   "source": [
    "plot_training_curves(\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(val_loader))\n",
    "preds = None\n",
    "vit.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    preds = outputs.argmax(dim=1)\n",
    "\n",
    "    # Move data to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = preds.cpu()\n",
    "\n",
    "\n",
    "visualize_images_with_labels(images, labels, idx_to_class, preds, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Appendix](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[Position Interplotation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZXPXvSfKPU-"
   },
   "outputs": [],
   "source": [
    "config = ModelConfig()\n",
    "vit = ViT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Kbc7DftKOTF"
   },
   "outputs": [],
   "source": [
    "orginal_image = torch.randn(8, 3, config.image_size, config.image_size)\n",
    "original_out = vit(orginal_image)\n",
    "\n",
    "assert original_out.shape == (8, config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRWEYTLAKel-"
   },
   "outputs": [],
   "source": [
    "def interpolate_pos_embed(pretrained_embed, new_grid_size, num_extra_tokens=1):\n",
    "    cls_token = pretrained_embed[:, :num_extra_tokens]\n",
    "    patch_pos_embed = pretrained_embed[:, num_extra_tokens:]\n",
    "\n",
    "    old_size = int(patch_pos_embed.shape[1] ** 0.5)\n",
    "    patch_pos_embed = patch_pos_embed.reshape(1, old_size, old_size, -1).permute(\n",
    "        0, 3, 1, 2\n",
    "    )\n",
    "\n",
    "    # use bicubic to interpolate\n",
    "    interpolated = F.interpolate(\n",
    "        patch_pos_embed, size=new_grid_size, mode=\"bicubic\", align_corners=False\n",
    "    )\n",
    "\n",
    "    interpolated = interpolated.permute(0, 2, 3, 1).reshape(\n",
    "        1, -1, pretrained_embed.shape[-1]\n",
    "    )\n",
    "\n",
    "    new_pos_embed = torch.cat((cls_token, interpolated), dim=1)\n",
    "    return new_pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utlCgesYLJag"
   },
   "outputs": [],
   "source": [
    "pretrain_pe = vit.positional_encoding.positional_embedding\n",
    "fine_tune_pe = interpolate_pos_embed(pretrain_pe, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtTU4dqWMDm_"
   },
   "outputs": [],
   "source": [
    "# Replace the positional encoding weight, and also enable fine tunine\n",
    "vit.positional_encoding.positional_embedding = nn.Parameter(\n",
    "    fine_tune_pe, requires_grad=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbcaiP9CMQfS"
   },
   "outputs": [],
   "source": [
    "fine_tune_image = torch.randn(8, 3, 384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the number of classification is same,\n",
    "# else, also need to replace the classifcation head(MLP layer)\n",
    "fine_tune_out = vit(fine_tune_image)\n",
    "assert fine_tune_out == (8, config.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Visualizing the Image Patch](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHmdwPgVMgpH"
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import einops\n",
    "\n",
    "# # Step 1: Load and resize image to 256x256\n",
    "# image_path = \"github.png\"\n",
    "# img_bgr = cv2.imread(image_path)\n",
    "# img_resized = cv2.resize(img_bgr, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "# img = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)  # Convert to RGB for display\n",
    "\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f_MDlpLlNB_"
   },
   "outputs": [],
   "source": [
    "# # Step 2: Patch the image using einops\n",
    "# patch_size = 16\n",
    "# patches = einops.rearrange(\n",
    "#     img, \"(h ph) (w pw) c -> (h w) ph pw c\", ph=patch_size, pw=patch_size\n",
    "# )\n",
    "\n",
    "# # Step 3: Display patches in their original grid layout\n",
    "# h, w = 256 // patch_size, 256 // patch_size\n",
    "# fig, axes = plt.subplots(h, w, figsize=(12, 12))\n",
    "# for i in range(h):\n",
    "#     for j in range(w):\n",
    "#         patch_idx = i * w + j\n",
    "#         axes[i, j].imshow(patches[patch_idx])\n",
    "#         axes[i, j].axis(\"off\")\n",
    "\n",
    "# plt.suptitle(\"Patches shown in original spatial distribution (16x16)\", fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
