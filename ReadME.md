## About this repository

This repository contains the code for the website of [100 AI Papers with Code(PwC)](https://yuyang.info/100-AI-Papers/)
[![The preview of the website](assets/website.png)](https://yuyang.info/100-AI-Papers/)

Below are list of the papers:
| Number | Paper Name | Description | Code | Blog | Recommendation |
| --- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |--------------------------------------------------------------------------------- | ----------------------------------------------------------------- | --------------- |
| 01 | [Attention is All You Need ](https://arxiv.org/abs/1706.03762) (ğŸ‘¾ **Transformer** ğŸ‘¾)| å¼•å…¥äº† Transformer æ¶æ„ï¼Œè¯¥æ¶æ„å®Œå…¨ä¾èµ– è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ è¿›è¡Œåºåˆ—å»ºæ¨¡ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥ å¹¶è¡Œè®¡ç®—ï¼Œå¹¶åœ¨ è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ ä¸Šå¤§å¹…æå‡äº†æ€§èƒ½ã€‚| [Code](https://github.com/YYZhang2025/100-AI-Code/blob/main/01_transformer.ipynb) | [Blog](https://yuyang.info/100-AI-Papers/posts/01-transformer.html) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ |
| 02 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (ğŸ‘¾ **Vision Transformer** ğŸ‘¾)| Vision Transformer (ViT) æ˜¯ä¸€ç§å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸º token è¾“å…¥æ ‡å‡† Transformer æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»çš„æ¶æ„ï¼Œ<u>é¦–æ¬¡å®ç°äº†çº¯æ³¨æ„åŠ›æœºåˆ¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨</u>ã€‚| [Code](https://github.com/YYZhang2025/100-AI-Code/blob/main/02_vision_transformer.ipynb) | [Blog](https://yuyang.info/100-AI-Papers/posts/02-vision-transformer.html) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ |
| 03 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) (ğŸ‘¾ **Swin Transformer** ğŸ‘¾)| Swin Transformer æ˜¯ä¸€ç§ä½¿ç”¨**å±‚æ¬¡åŒ–ç»“æ„**å’Œ**æ»‘åŠ¨çª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶**çš„è§†è§‰ Transformer æ¨¡å‹ï¼Œ<u>æ—¢ä¿ç•™äº†å±€éƒ¨å»ºæ¨¡çš„é«˜æ•ˆæ€§ï¼Œåˆé€šè¿‡çª—å£åç§»å®ç°è·¨åŒºåŸŸä¿¡æ¯äº¤äº’</u>ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ã€‚ | [Code](https://github.com/YYZhang2025/100-AI-Code/blob/main/03_swin_transformer.ipynb) | [Blog](https://yuyang.info/100-AI-Papers/posts/03-swin-transformer.html) | â­ï¸â­ï¸â­ï¸ |
| 04 | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (ğŸ‘¾ **CLIP** ğŸ‘¾)| CLIP æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡å›¾æ–‡**å¯¹æ¯”å­¦ä¹ **ï¼Œå°†å›¾åƒä¸è‡ªç„¶è¯­è¨€æ˜ å°„åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬å›¾åƒè¯†åˆ«ä¸è·¨æ¨¡æ€æ£€ç´¢çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ | [Code](https://github.com/YYZhang2025/100-AI-Code/blob/main/04_clip.ipynb) | [Blog](https://yuyang.info/100-AI-Papers/posts/04-clip.html) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |
| 05 | [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2103.00020) (ğŸ‘¾ **Flash Attention** ğŸ‘¾)| FlashAttention æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œé€šè¿‡å‡å°‘å†…å­˜è®¿é—®å’Œæå‡è®¡ç®—æ•ˆç‡ï¼Œå®ç°æ›´å¿«ã€æ›´èŠ‚çœèµ„æºçš„ Transformer æ¨ç†ä¸è®­ç»ƒã€‚ | [Code](https://github.com/YYZhang2025/100-AI-Code/blob/main/05_flash_attention.ipynb) | [Blog](https://yuyang.info/100-AI-Papers/posts/05-flash-attention.html) | â­ï¸â­ï¸â­ï¸  |
